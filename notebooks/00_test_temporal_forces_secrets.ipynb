{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from envs.secret.secret_envs_wrapper import SecretEnv0, SecretEnv1, SecretEnv2, SecretEnv3\n",
    "from rl_algorithms.temporal_forces_secret.expected_sarsa import expected_sarsa_blackbox\n",
    "\n",
    "env = SecretEnv0()\n",
    "Q_esarsa, rewards_esarsa, lengths_esarsa = expected_sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards_esarsa[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards_esarsa[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ae517",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SecretEnv1()\n",
    "Q_esarsa, rewards_esarsa, lengths_esarsa = expected_sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards_esarsa[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards_esarsa[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SecretEnv2()\n",
    "Q_esarsa, rewards_esarsa, lengths_esarsa = expected_sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards_esarsa[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards_esarsa[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SecretEnv3()\n",
    "Q_esarsa, rewards_esarsa, lengths_esarsa = expected_sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards_esarsa[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards_esarsa[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv0\n",
    "from rl_algorithms.temporal_forces_secret.q_learning import q_learning_blackbox\n",
    "\n",
    "env = SecretEnv0()\n",
    "Q, rewards, lengths = q_learning_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv1\n",
    "\n",
    "env = SecretEnv1()\n",
    "Q, rewards, lengths = q_learning_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c068de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv2\n",
    "\n",
    "env = SecretEnv2()\n",
    "Q, rewards, lengths = q_learning_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv3\n",
    "\n",
    "env = SecretEnv3()\n",
    "Q, rewards, lengths = q_learning_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv0\n",
    "from rl_algorithms.temporal_forces_secret.sarsa import sarsa_blackbox\n",
    "\n",
    "env = SecretEnv0()\n",
    "Q, rewards, lengths = sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv1\n",
    "\n",
    "env = SecretEnv1()\n",
    "Q, rewards, lengths = sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv2\n",
    "\n",
    "env = SecretEnv2()\n",
    "Q, rewards, lengths = sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06297ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.secret.secret_envs_wrapper import SecretEnv3\n",
    "\n",
    "env = SecretEnv3()\n",
    "Q, rewards, lengths = sarsa_blackbox(\n",
    "    env, episodes=5000, alpha=0.1, gamma=0.99, epsilon=0.1, verbose=True\n",
    ")\n",
    "print(f\"\\nMean final reward: {np.mean(rewards[-100:]):.4f}\")\n",
    "print(f\"Std of final rewards: {np.std(rewards[-100:]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
