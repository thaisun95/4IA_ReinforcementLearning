{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e47181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC ES policy: [0 1 1 1 0]\n",
      "On-policy MC policy: [0 1 1 1 0]\n",
      "Off-policy MC policy: [0 1 1 1 0]\n",
      "\n",
      "MC ES:\n",
      "Policy: ← → → → ←\n",
      "\n",
      "On-policy MC:\n",
      "Policy: ← → → → ←\n",
      "\n",
      "Off-policy MC:\n",
      "Policy: ← → → → ←\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from envs.line_world.line_world import LineWorld\n",
    "from rl_algorithms.mc_algorithms.mc_es import mc_es\n",
    "from rl_algorithms.mc_algorithms.mc_on_policy import mc_on_policy\n",
    "from rl_algorithms.mc_algorithms.mc_off_policy import mc_off_policy\n",
    "\n",
    "env = LineWorld(size=5, start_state=2)\n",
    "\n",
    "# --- MC ES\n",
    "Q_es, policy_es = mc_es(env, num_episodes=5000, gamma=0.99, max_steps_per_episode=None, verbose=False)\n",
    "print(\"MC ES policy:\", policy_es)\n",
    "\n",
    "# --- On-policy MC\n",
    "Q_on, policy_on = mc_on_policy(env, num_episodes=5000, gamma=0.99, epsilon=0.1, max_steps_per_episode=None, verbose=False)\n",
    "print(\"On-policy MC policy:\", policy_on.argmax(axis=1) if policy_on.ndim==2 else policy_on)\n",
    "\n",
    "# --- Off-policy MC\n",
    "Q_off, policy_off = mc_off_policy(env, num_episodes=5000, gamma=0.99, max_steps_per_episode=None, verbose=False)\n",
    "print(\"Off-policy MC policy:\", policy_off)\n",
    "\n",
    "# --- Visualize (as arrows)\n",
    "def print_policy_lineworld(policy):\n",
    "    action_map = {0: \"←\", 1: \"→\", -1: \"X\"}\n",
    "    print(\"Policy:\", ' '.join([action_map.get(a, str(a)) for a in policy]))\n",
    "\n",
    "print(\"\\nMC ES:\")\n",
    "print_policy_lineworld(policy_es)\n",
    "print(\"\\nOn-policy MC:\")\n",
    "print_policy_lineworld(policy_on.argmax(axis=1) if policy_on.ndim==2 else policy_on)\n",
    "print(\"\\nOff-policy MC:\")\n",
    "print_policy_lineworld(policy_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63869d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid episodes (terminated): 10000/10000\n",
      "Valid episodes (terminated): 9970/10000\n",
      "Valid episodes (terminated): 10000/10000\n",
      "MC ES Policy:\n",
      "→ ↓ ↓ ↓\n",
      "↓ → ↓ ↓\n",
      "→ → ↓ ↓\n",
      "→ → → ↑\n",
      "\n",
      "On-policy MC Policy:\n",
      "↓ ← ↓ ↓\n",
      "→ ↓ ↓ ↓\n",
      "↓ → → ↓\n",
      "→ → → ↑\n",
      "\n",
      "Off-policy MC Policy:\n",
      "↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑\n",
      "↑ ↑ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from envs.grid_world.grid_world import GridWorld\n",
    "from rl_algorithms.mc_algorithms.mc_es import mc_es\n",
    "from rl_algorithms.mc_algorithms.mc_on_policy import mc_on_policy\n",
    "from rl_algorithms.mc_algorithms.mc_off_policy import mc_off_policy\n",
    "\n",
    "env = GridWorld(n_rows=4, n_cols=4, start_state=(0, 0))\n",
    "\n",
    "Q_es, policy_es = mc_es(env, num_episodes=10000, gamma=0.99, max_steps_per_episode=400, verbose=True)\n",
    "Q_on, policy_on = mc_on_policy(env, num_episodes=10000, gamma=0.99, epsilon=0.1, max_steps_per_episode=400, verbose=True)\n",
    "Q_off, policy_off = mc_off_policy(env, num_episodes=10000, gamma=0.99, max_steps_per_episode=400, verbose=True)\n",
    "\n",
    "# --- Policy visualization as arrows on the grid:\n",
    "def render_policy_grid(env, policy):\n",
    "    arrow_map = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\", -1: \"X\"}\n",
    "    grid = np.array([[arrow_map[policy[env.state_to_index((r, c))]]\n",
    "                     for c in range(env.n_cols)]\n",
    "                    for r in range(env.n_rows)])\n",
    "    print('\\n'.join(' '.join(row) for row in grid))\n",
    "\n",
    "print(\"MC ES Policy:\")\n",
    "render_policy_grid(env, policy_es)\n",
    "print(\"\\nOn-policy MC Policy:\")\n",
    "render_policy_grid(env, policy_on.argmax(axis=1) if policy_on.ndim == 2 else policy_on)\n",
    "print(\"\\nOff-policy MC Policy:\")\n",
    "render_policy_grid(env, policy_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3079701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc_es : Valid episodes = 0/10000\n",
      "on_policy : Valid episodes = 8283/10000\n",
      "off_policy : Valid episodes = 8990/10000\n",
      "MC ES Policy:\n",
      "↑ ↑ ↑\n",
      "↑ ↑ ↑\n",
      "↑ ↑ ↑\n",
      "\n",
      "On-policy MC Policy:\n",
      "↑ ↓ ↓\n",
      "↑ ← ↓\n",
      "→ → ↑\n",
      "\n",
      "Off-policy MC Policy:\n",
      "↓ ↓ ↓\n",
      "→ ↓ ↓\n",
      "→ → ↑\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from envs.grid_world.grid_world import GridWorld\n",
    "from rl_algorithms.mc_algorithms.mc_es import mc_es\n",
    "from rl_algorithms.mc_algorithms.mc_on_policy import mc_on_policy\n",
    "from rl_algorithms.mc_algorithms.mc_off_policy import mc_off_policy\n",
    "\n",
    "env = GridWorld(n_rows=3, n_cols=3, start_state=(0, 0))\n",
    "\n",
    "def run_mc_algo_with_random_start(mc_algo, algo_name, num_episodes=10000, gamma=0.99, epsilon=0.1, max_steps=50):\n",
    "    n_states = env.n_states\n",
    "\n",
    "    # New version : starting in a non terminate state\n",
    "    def reset_random():\n",
    "        possible_states = [i for i in range(n_states) if not env.is_terminal(env.index_to_state(i))]\n",
    "        s_idx = random.choice(possible_states)\n",
    "        s = env.index_to_state(s_idx)\n",
    "        env.state = s\n",
    "        return s\n",
    "\n",
    "    Q = np.zeros((n_states, len(env.action_space)))\n",
    "    policy = None\n",
    "\n",
    "    valid_episodes = 0\n",
    "    for episode in range(num_episodes):\n",
    "        s = reset_random()\n",
    "        episode_list = []\n",
    "        steps = 0\n",
    "        done = False\n",
    "        while not done and steps < max_steps:\n",
    "            s_idx = env.state_to_index(s)\n",
    "            if algo_name == 'on_policy':\n",
    "                a = np.argmax(Q[s_idx]) if random.random() > epsilon else random.choice(env.action_space)\n",
    "            elif algo_name == 'off_policy':\n",
    "                a = random.choice(env.action_space)\n",
    "            else: # MC ES, greedy\n",
    "                a = np.argmax(Q[s_idx])\n",
    "            next_s, reward, done = env.simulate_step(s, a)\n",
    "            episode_list.append((s_idx, a, reward))\n",
    "            s = next_s\n",
    "            steps += 1\n",
    "        if env.is_terminal(s):\n",
    "            valid_episodes += 1\n",
    "            G = 0\n",
    "            visited = set()\n",
    "            for t in reversed(range(len(episode_list))):\n",
    "                s_idx, a, r = episode_list[t]\n",
    "                G = gamma * G + r\n",
    "                if (s_idx, a) not in visited:\n",
    "                    Q[s_idx, a] += (G - Q[s_idx, a]) / (1 + episode)\n",
    "                    visited.add((s_idx, a))\n",
    "    # Derive policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    print(f\"{algo_name} : Valid episodes = {valid_episodes}/{num_episodes}\")\n",
    "    return Q, policy\n",
    "\n",
    "# Run MC ES, On-policy, Off-policy\n",
    "Q_es, policy_es = run_mc_algo_with_random_start(mc_es, 'mc_es', num_episodes=10000)\n",
    "Q_on, policy_on = run_mc_algo_with_random_start(mc_on_policy, 'on_policy', num_episodes=10000, epsilon=0.1)\n",
    "Q_off, policy_off = run_mc_algo_with_random_start(mc_off_policy, 'off_policy', num_episodes=10000)\n",
    "\n",
    "def render_policy_grid(env, policy):\n",
    "    arrow_map = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\", -1: \"X\"}\n",
    "    grid = np.array([[arrow_map[policy[env.state_to_index((r, c))]]\n",
    "                     for c in range(env.n_cols)]\n",
    "                    for r in range(env.n_rows)])\n",
    "    print('\\n'.join(' '.join(row) for row in grid))\n",
    "\n",
    "print(\"MC ES Policy:\")\n",
    "render_policy_grid(env, policy_es)\n",
    "print(\"\\nOn-policy MC Policy:\")\n",
    "render_policy_grid(env, policy_on)\n",
    "print(\"\\nOff-policy MC Policy:\")\n",
    "render_policy_grid(env, policy_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520ccb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid episodes (terminated): 10000/10000\n",
      "MC ES policy: [0 1 2 0 0 0 0]\n",
      "Valid episodes (terminated): 10000/10000\n",
      "On-policy MC policy: [2 1 2 0 0 0 0]\n",
      "Valid episodes (terminated): 10000/10000\n",
      "Off-policy MC policy: [2 1 2 0 0 0 0]\n",
      "\n",
      "MC ES:\n",
      "Policy:\n",
      "State (0, -1): Rock\n",
      "State (1, 0): Paper\n",
      "State (1, 1): Scissors\n",
      "State (1, 2): Rock\n",
      "State (2, 0): Rock\n",
      "State (2, 1): Rock\n",
      "State (2, 2): Rock\n",
      "\n",
      "On-policy MC:\n",
      "Policy:\n",
      "State (0, -1): Scissors\n",
      "State (1, 0): Paper\n",
      "State (1, 1): Scissors\n",
      "State (1, 2): Rock\n",
      "State (2, 0): Rock\n",
      "State (2, 1): Rock\n",
      "State (2, 2): Rock\n",
      "\n",
      "Off-policy MC:\n",
      "Policy:\n",
      "State (0, -1): Scissors\n",
      "State (1, 0): Paper\n",
      "State (1, 1): Scissors\n",
      "State (1, 2): Rock\n",
      "State (2, 0): Rock\n",
      "State (2, 1): Rock\n",
      "State (2, 2): Rock\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from envs.rps.two_round_rps import TwoRoundRPS\n",
    "from rl_algorithms.mc_algorithms.mc_es import mc_es\n",
    "from rl_algorithms.mc_algorithms.mc_on_policy import mc_on_policy\n",
    "from rl_algorithms.mc_algorithms.mc_off_policy import mc_off_policy\n",
    "\n",
    "env = TwoRoundRPS()\n",
    "\n",
    "# Monte Carlo ES\n",
    "Q_es, policy_es = mc_es(env, num_episodes=10000, gamma=0.99, verbose=True)\n",
    "print(\"MC ES policy:\", policy_es)\n",
    "\n",
    "# On-policy MC\n",
    "Q_on, policy_on = mc_on_policy(env, num_episodes=10000, gamma=0.99, epsilon=0.1, verbose=True)\n",
    "print(\"On-policy MC policy:\", policy_on.argmax(axis=1) if policy_on.ndim==2 else policy_on)\n",
    "\n",
    "# Off-policy MC\n",
    "Q_off, policy_off = mc_off_policy(env, num_episodes=10000, gamma=0.99, verbose=True)\n",
    "print(\"Off-policy MC policy:\", policy_off)\n",
    "\n",
    "def print_policy_rps(env, policy):\n",
    "    action_map = {0: \"Rock\", 1: \"Paper\", 2: \"Scissors\", -1: \"X\"}\n",
    "    print(\"Policy:\")\n",
    "    for idx, state in enumerate(env.states):\n",
    "        print(f\"State {state}: {action_map.get(policy[idx], str(policy[idx]))}\")\n",
    "\n",
    "print(\"\\nMC ES:\")\n",
    "print_policy_rps(env, policy_es)\n",
    "print(\"\\nOn-policy MC:\")\n",
    "print_policy_rps(env, policy_on.argmax(axis=1) if policy_on.ndim==2 else policy_on)\n",
    "print(\"\\nOff-policy MC:\")\n",
    "print_policy_rps(env, policy_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cd34cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid episodes (terminated): 20000/20000\n",
      "Valid episodes (terminated): 20000/20000\n",
      "Valid episodes (terminated): 20000/20000\n",
      "\n",
      "MC ES:\n",
      "Policy:\n",
      "State (0, -1, -1): Choose0/Stay\n",
      "State (1, 0, 1): Choose0/Stay\n",
      "State (1, 0, 2): Choose0/Stay\n",
      "State (1, 1, 0): Choose0/Stay\n",
      "State (1, 1, 2): Choose0/Stay\n",
      "State (1, 2, 0): Choose0/Stay\n",
      "State (1, 2, 1): Choose0/Stay\n",
      "State (2, 0, -1): Choose0/Stay\n",
      "State (2, 1, -1): Choose0/Stay\n",
      "State (2, 2, -1): Choose0/Stay\n",
      "\n",
      "On-policy MC:\n",
      "Policy:\n",
      "State (0, -1, -1): Choose0/Stay\n",
      "State (1, 0, 1): Choose1/Switch\n",
      "State (1, 0, 2): Choose1/Switch\n",
      "State (1, 1, 0): Choose1/Switch\n",
      "State (1, 1, 2): Choose1/Switch\n",
      "State (1, 2, 0): Choose1/Switch\n",
      "State (1, 2, 1): Choose1/Switch\n",
      "State (2, 0, -1): Choose0/Stay\n",
      "State (2, 1, -1): Choose0/Stay\n",
      "State (2, 2, -1): Choose0/Stay\n",
      "\n",
      "Off-policy MC:\n",
      "Policy:\n",
      "State (0, -1, -1): Choose2\n",
      "State (1, 0, 1): Choose1/Switch\n",
      "State (1, 0, 2): Choose1/Switch\n",
      "State (1, 1, 0): Choose1/Switch\n",
      "State (1, 1, 2): Choose1/Switch\n",
      "State (1, 2, 0): Choose1/Switch\n",
      "State (1, 2, 1): Choose1/Switch\n",
      "State (2, 0, -1): Choose0/Stay\n",
      "State (2, 1, -1): Choose0/Stay\n",
      "State (2, 2, -1): Choose0/Stay\n"
     ]
    }
   ],
   "source": [
    "from envs.monty_hall.monty_hall_v1 import MontyHallV1\n",
    "\n",
    "env = MontyHallV1()\n",
    "\n",
    "Q_es, policy_es = mc_es(env, num_episodes=20000, gamma=0.99, verbose=True)\n",
    "Q_on, policy_on = mc_on_policy(env, num_episodes=20000, gamma=0.99, epsilon=0.1, verbose=True)\n",
    "Q_off, policy_off = mc_off_policy(env, num_episodes=20000, gamma=0.99, verbose=True)\n",
    "\n",
    "def print_policy_monty(env, policy):\n",
    "    action_map = {0: \"Choose0/Stay\", 1: \"Choose1/Switch\", 2: \"Choose2\", 3: \"Stay\", 4: \"Switch\", -1: \"X\"}\n",
    "    print(\"Policy:\")\n",
    "    for idx, state in enumerate(env.states):\n",
    "        print(f\"State {state}: {action_map.get(policy[idx], str(policy[idx]))}\")\n",
    "\n",
    "print(\"\\nMC ES:\")\n",
    "print_policy_monty(env, policy_es)\n",
    "print(\"\\nOn-policy MC:\")\n",
    "print_policy_monty(env, policy_on.argmax(axis=1) if policy_on.ndim==2 else policy_on)\n",
    "print(\"\\nOff-policy MC:\")\n",
    "print_policy_monty(env, policy_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873f33d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid episodes (terminated): 40000/40000\n",
      "Valid episodes (terminated): 40000/40000\n",
      "Valid episodes (terminated): 40000/40000\n",
      "\n",
      "MC ES:\n",
      "Policy:\n",
      "State (0, (0, 1, 2, 3, 4), -1): action 0\n",
      "State (1, (0, 2, 3, 4), 0): action 0\n",
      "State (1, (0, 1, 3, 4), 0): action 0\n",
      "State (1, (0, 1, 2, 4), 0): action 0\n",
      "State (1, (0, 1, 2, 3), 0): action 0\n",
      "State (1, (0, 1, 3, 4), 1): action 0\n",
      "State (1, (0, 1, 2, 4), 1): action 0\n",
      "State (1, (0, 1, 2, 3), 1): action 0\n",
      "State (1, (1, 2, 3, 4), 1): action 1\n",
      "State (1, (0, 2, 3, 4), 2): action 0\n",
      "State (1, (0, 1, 2, 4), 2): action 0\n",
      "State (1, (0, 1, 2, 3), 2): action 0\n",
      "State (1, (1, 2, 3, 4), 2): action 1\n",
      "State (1, (0, 2, 3, 4), 3): action 0\n",
      "State (1, (0, 1, 3, 4), 3): action 0\n",
      "State (1, (0, 1, 2, 3), 3): action 0\n",
      "State (1, (1, 2, 3, 4), 3): action 1\n",
      "State (1, (0, 2, 3, 4), 4): action 0\n",
      "State (1, (0, 1, 3, 4), 4): action 0\n",
      "State (1, (0, 1, 2, 4), 4): action 0\n",
      "State (1, (1, 2, 3, 4), 4): action 1\n",
      "State (2, (0, 3, 4), 0): action 0\n",
      "State (2, (0, 2, 4), 0): action 0\n",
      "State (2, (0, 2, 3), 0): action 0\n",
      "State (2, (0, 2, 4), 2): action 0\n",
      "State (2, (0, 2, 3), 2): action 0\n",
      "State (2, (2, 3, 4), 2): action 2\n",
      "State (2, (0, 3, 4), 3): action 0\n",
      "State (2, (0, 2, 3), 3): action 0\n",
      "State (2, (2, 3, 4), 3): action 2\n",
      "State (2, (0, 3, 4), 4): action 0\n",
      "State (2, (0, 2, 4), 4): action 0\n",
      "State (2, (2, 3, 4), 4): action 2\n",
      "State (2, (0, 1, 4), 0): action 0\n",
      "State (2, (0, 1, 3), 0): action 0\n",
      "State (2, (0, 1, 4), 1): action 0\n",
      "State (2, (0, 1, 3), 1): action 0\n",
      "State (2, (1, 3, 4), 1): action 1\n",
      "State (2, (0, 1, 3), 3): action 0\n",
      "State (2, (1, 3, 4), 3): action 1\n",
      "State (2, (0, 1, 4), 4): action 0\n",
      "State (2, (1, 3, 4), 4): action 1\n",
      "State (2, (0, 1, 2), 0): action 0\n",
      "State (2, (0, 1, 2), 1): action 0\n",
      "State (2, (1, 2, 4), 1): action 1\n",
      "State (2, (0, 1, 2), 2): action 0\n",
      "State (2, (1, 2, 4), 2): action 1\n",
      "State (2, (1, 2, 4), 4): action 1\n",
      "State (2, (1, 2, 3), 1): action 1\n",
      "State (2, (1, 2, 3), 2): action 1\n",
      "State (2, (1, 2, 3), 3): action 1\n",
      "State (3, (0, 4), 0): action 0\n",
      "State (3, (0, 3), 0): action 0\n",
      "State (3, (0, 3), 3): action 0\n",
      "State (3, (3, 4), 3): action 3\n",
      "State (3, (0, 4), 4): action 0\n",
      "State (3, (3, 4), 4): action 3\n",
      "State (3, (0, 2), 0): action 0\n",
      "State (3, (0, 2), 2): action 0\n",
      "State (3, (2, 4), 2): action 2\n",
      "State (3, (2, 4), 4): action 2\n",
      "State (3, (2, 3), 2): action 2\n",
      "State (3, (2, 3), 3): action 2\n",
      "State (3, (0, 1), 0): action 0\n",
      "State (3, (0, 1), 1): action 0\n",
      "State (3, (1, 4), 1): action 1\n",
      "State (3, (1, 4), 4): action 1\n",
      "State (3, (1, 3), 1): action 1\n",
      "State (3, (1, 3), 3): action 1\n",
      "State (3, (1, 2), 1): action 1\n",
      "State (3, (1, 2), 2): action 1\n",
      "State (5, (0, 4), 0): action 0\n",
      "State (5, (0, 4), 4): action 0\n",
      "State (5, (0, 3), 0): action 0\n",
      "State (5, (0, 3), 3): action 0\n",
      "State (5, (3, 4), 3): action 0\n",
      "State (5, (3, 4), 4): action 0\n",
      "State (5, (0, 2), 0): action 0\n",
      "State (5, (0, 2), 2): action 0\n",
      "State (5, (2, 4), 2): action 0\n",
      "State (5, (2, 4), 4): action 0\n",
      "State (5, (2, 3), 2): action 0\n",
      "State (5, (2, 3), 3): action 0\n",
      "State (5, (0, 1), 0): action 0\n",
      "State (5, (0, 1), 1): action 0\n",
      "State (5, (1, 4), 1): action 0\n",
      "State (5, (1, 4), 4): action 0\n",
      "State (5, (1, 3), 1): action 0\n",
      "State (5, (1, 3), 3): action 0\n",
      "State (5, (1, 2), 1): action 0\n",
      "State (5, (1, 2), 2): action 0\n",
      "\n",
      "On-policy MC:\n",
      "Policy:\n",
      "State (0, (0, 1, 2, 3, 4), -1): action 3\n",
      "State (1, (0, 2, 3, 4), 0): action 0\n",
      "State (1, (0, 1, 3, 4), 0): action 3\n",
      "State (1, (0, 1, 2, 4), 0): action 4\n",
      "State (1, (0, 1, 2, 3), 0): action 1\n",
      "State (1, (0, 1, 3, 4), 1): action 4\n",
      "State (1, (0, 1, 2, 4), 1): action 4\n",
      "State (1, (0, 1, 2, 3), 1): action 1\n",
      "State (1, (1, 2, 3, 4), 1): action 2\n",
      "State (1, (0, 2, 3, 4), 2): action 3\n",
      "State (1, (0, 1, 2, 4), 2): action 1\n",
      "State (1, (0, 1, 2, 3), 2): action 3\n",
      "State (1, (1, 2, 3, 4), 2): action 2\n",
      "State (1, (0, 2, 3, 4), 3): action 3\n",
      "State (1, (0, 1, 3, 4), 3): action 3\n",
      "State (1, (0, 1, 2, 3), 3): action 3\n",
      "State (1, (1, 2, 3, 4), 3): action 3\n",
      "State (1, (0, 2, 3, 4), 4): action 0\n",
      "State (1, (0, 1, 3, 4), 4): action 3\n",
      "State (1, (0, 1, 2, 4), 4): action 0\n",
      "State (1, (1, 2, 3, 4), 4): action 3\n",
      "State (2, (0, 3, 4), 0): action 4\n",
      "State (2, (0, 2, 4), 0): action 4\n",
      "State (2, (0, 2, 3), 0): action 2\n",
      "State (2, (0, 2, 4), 2): action 0\n",
      "State (2, (0, 2, 3), 2): action 2\n",
      "State (2, (2, 3, 4), 2): action 2\n",
      "State (2, (0, 3, 4), 3): action 3\n",
      "State (2, (0, 2, 3), 3): action 3\n",
      "State (2, (2, 3, 4), 3): action 3\n",
      "State (2, (0, 3, 4), 4): action 4\n",
      "State (2, (0, 2, 4), 4): action 4\n",
      "State (2, (2, 3, 4), 4): action 2\n",
      "State (2, (0, 1, 4), 0): action 4\n",
      "State (2, (0, 1, 3), 0): action 3\n",
      "State (2, (0, 1, 4), 1): action 1\n",
      "State (2, (0, 1, 3), 1): action 1\n",
      "State (2, (1, 3, 4), 1): action 4\n",
      "State (2, (0, 1, 3), 3): action 3\n",
      "State (2, (1, 3, 4), 3): action 3\n",
      "State (2, (0, 1, 4), 4): action 4\n",
      "State (2, (1, 3, 4), 4): action 1\n",
      "State (2, (0, 1, 2), 0): action 2\n",
      "State (2, (0, 1, 2), 1): action 1\n",
      "State (2, (1, 2, 4), 1): action 1\n",
      "State (2, (0, 1, 2), 2): action 0\n",
      "State (2, (1, 2, 4), 2): action 1\n",
      "State (2, (1, 2, 4), 4): action 4\n",
      "State (2, (1, 2, 3), 1): action 2\n",
      "State (2, (1, 2, 3), 2): action 3\n",
      "State (2, (1, 2, 3), 3): action 3\n",
      "State (3, (0, 4), 0): action 4\n",
      "State (3, (0, 3), 0): action 3\n",
      "State (3, (0, 3), 3): action 0\n",
      "State (3, (3, 4), 3): action 4\n",
      "State (3, (0, 4), 4): action 0\n",
      "State (3, (3, 4), 4): action 3\n",
      "State (3, (0, 2), 0): action 2\n",
      "State (3, (0, 2), 2): action 0\n",
      "State (3, (2, 4), 2): action 4\n",
      "State (3, (2, 4), 4): action 2\n",
      "State (3, (2, 3), 2): action 3\n",
      "State (3, (2, 3), 3): action 2\n",
      "State (3, (0, 1), 0): action 1\n",
      "State (3, (0, 1), 1): action 0\n",
      "State (3, (1, 4), 1): action 4\n",
      "State (3, (1, 4), 4): action 1\n",
      "State (3, (1, 3), 1): action 3\n",
      "State (3, (1, 3), 3): action 1\n",
      "State (3, (1, 2), 1): action 2\n",
      "State (3, (1, 2), 2): action 1\n",
      "State (5, (0, 4), 0): action 0\n",
      "State (5, (0, 4), 4): action 0\n",
      "State (5, (0, 3), 0): action 0\n",
      "State (5, (0, 3), 3): action 0\n",
      "State (5, (3, 4), 3): action 0\n",
      "State (5, (3, 4), 4): action 0\n",
      "State (5, (0, 2), 0): action 0\n",
      "State (5, (0, 2), 2): action 0\n",
      "State (5, (2, 4), 2): action 0\n",
      "State (5, (2, 4), 4): action 0\n",
      "State (5, (2, 3), 2): action 0\n",
      "State (5, (2, 3), 3): action 0\n",
      "State (5, (0, 1), 0): action 0\n",
      "State (5, (0, 1), 1): action 0\n",
      "State (5, (1, 4), 1): action 0\n",
      "State (5, (1, 4), 4): action 0\n",
      "State (5, (1, 3), 1): action 0\n",
      "State (5, (1, 3), 3): action 0\n",
      "State (5, (1, 2), 1): action 0\n",
      "State (5, (1, 2), 2): action 0\n",
      "\n",
      "Off-policy MC:\n",
      "Policy:\n",
      "State (0, (0, 1, 2, 3, 4), -1): action 0\n",
      "State (1, (0, 2, 3, 4), 0): action 0\n",
      "State (1, (0, 1, 3, 4), 0): action 0\n",
      "State (1, (0, 1, 2, 4), 0): action 0\n",
      "State (1, (0, 1, 2, 3), 0): action 3\n",
      "State (1, (0, 1, 3, 4), 1): action 4\n",
      "State (1, (0, 1, 2, 4), 1): action 4\n",
      "State (1, (0, 1, 2, 3), 1): action 1\n",
      "State (1, (1, 2, 3, 4), 1): action 3\n",
      "State (1, (0, 2, 3, 4), 2): action 3\n",
      "State (1, (0, 1, 2, 4), 2): action 1\n",
      "State (1, (0, 1, 2, 3), 2): action 2\n",
      "State (1, (1, 2, 3, 4), 2): action 2\n",
      "State (1, (0, 2, 3, 4), 3): action 4\n",
      "State (1, (0, 1, 3, 4), 3): action 3\n",
      "State (1, (0, 1, 2, 3), 3): action 2\n",
      "State (1, (1, 2, 3, 4), 3): action 3\n",
      "State (1, (0, 2, 3, 4), 4): action 4\n",
      "State (1, (0, 1, 3, 4), 4): action 4\n",
      "State (1, (0, 1, 2, 4), 4): action 4\n",
      "State (1, (1, 2, 3, 4), 4): action 4\n",
      "State (2, (0, 3, 4), 0): action 0\n",
      "State (2, (0, 2, 4), 0): action 0\n",
      "State (2, (0, 2, 3), 0): action 0\n",
      "State (2, (0, 2, 4), 2): action 2\n",
      "State (2, (0, 2, 3), 2): action 2\n",
      "State (2, (2, 3, 4), 2): action 2\n",
      "State (2, (0, 3, 4), 3): action 3\n",
      "State (2, (0, 2, 3), 3): action 3\n",
      "State (2, (2, 3, 4), 3): action 3\n",
      "State (2, (0, 3, 4), 4): action 4\n",
      "State (2, (0, 2, 4), 4): action 4\n",
      "State (2, (2, 3, 4), 4): action 4\n",
      "State (2, (0, 1, 4), 0): action 0\n",
      "State (2, (0, 1, 3), 0): action 0\n",
      "State (2, (0, 1, 4), 1): action 1\n",
      "State (2, (0, 1, 3), 1): action 1\n",
      "State (2, (1, 3, 4), 1): action 1\n",
      "State (2, (0, 1, 3), 3): action 3\n",
      "State (2, (1, 3, 4), 3): action 3\n",
      "State (2, (0, 1, 4), 4): action 4\n",
      "State (2, (1, 3, 4), 4): action 4\n",
      "State (2, (0, 1, 2), 0): action 0\n",
      "State (2, (0, 1, 2), 1): action 1\n",
      "State (2, (1, 2, 4), 1): action 1\n",
      "State (2, (0, 1, 2), 2): action 2\n",
      "State (2, (1, 2, 4), 2): action 2\n",
      "State (2, (1, 2, 4), 4): action 4\n",
      "State (2, (1, 2, 3), 1): action 1\n",
      "State (2, (1, 2, 3), 2): action 2\n",
      "State (2, (1, 2, 3), 3): action 3\n",
      "State (3, (0, 4), 0): action 4\n",
      "State (3, (0, 3), 0): action 3\n",
      "State (3, (0, 3), 3): action 0\n",
      "State (3, (3, 4), 3): action 4\n",
      "State (3, (0, 4), 4): action 0\n",
      "State (3, (3, 4), 4): action 3\n",
      "State (3, (0, 2), 0): action 2\n",
      "State (3, (0, 2), 2): action 0\n",
      "State (3, (2, 4), 2): action 4\n",
      "State (3, (2, 4), 4): action 2\n",
      "State (3, (2, 3), 2): action 3\n",
      "State (3, (2, 3), 3): action 2\n",
      "State (3, (0, 1), 0): action 1\n",
      "State (3, (0, 1), 1): action 0\n",
      "State (3, (1, 4), 1): action 4\n",
      "State (3, (1, 4), 4): action 1\n",
      "State (3, (1, 3), 1): action 3\n",
      "State (3, (1, 3), 3): action 1\n",
      "State (3, (1, 2), 1): action 2\n",
      "State (3, (1, 2), 2): action 1\n",
      "State (5, (0, 4), 0): action 0\n",
      "State (5, (0, 4), 4): action 0\n",
      "State (5, (0, 3), 0): action 0\n",
      "State (5, (0, 3), 3): action 0\n",
      "State (5, (3, 4), 3): action 0\n",
      "State (5, (3, 4), 4): action 0\n",
      "State (5, (0, 2), 0): action 0\n",
      "State (5, (0, 2), 2): action 0\n",
      "State (5, (2, 4), 2): action 0\n",
      "State (5, (2, 4), 4): action 0\n",
      "State (5, (2, 3), 2): action 0\n",
      "State (5, (2, 3), 3): action 0\n",
      "State (5, (0, 1), 0): action 0\n",
      "State (5, (0, 1), 1): action 0\n",
      "State (5, (1, 4), 1): action 0\n",
      "State (5, (1, 4), 4): action 0\n",
      "State (5, (1, 3), 1): action 0\n",
      "State (5, (1, 3), 3): action 0\n",
      "State (5, (1, 2), 1): action 0\n",
      "State (5, (1, 2), 2): action 0\n"
     ]
    }
   ],
   "source": [
    "from envs.monty_hall.monty_hall_v2 import MontyHallV2\n",
    "\n",
    "env = MontyHallV2(n_doors=5)  # 5 doors\n",
    "\n",
    "Q_es, policy_es = mc_es(env, num_episodes=40000, gamma=0.99, verbose=True)\n",
    "Q_on, policy_on = mc_on_policy(env, num_episodes=40000, gamma=0.99, epsilon=0.1, verbose=True)\n",
    "Q_off, policy_off = mc_off_policy(env, num_episodes=40000, gamma=0.99, verbose=True)\n",
    "\n",
    "def print_policy_monty_v2(env, policy):\n",
    "    print(\"Policy:\")\n",
    "    for idx, state in enumerate(env.states):\n",
    "        print(f\"State {state}: action {policy[idx]}\")\n",
    "\n",
    "print(\"\\nMC ES:\")\n",
    "print_policy_monty_v2(env, policy_es)\n",
    "print(\"\\nOn-policy MC:\")\n",
    "print_policy_monty_v2(env, policy_on.argmax(axis=1) if policy_on.ndim == 2 else policy_on)\n",
    "print(\"\\nOff-policy MC:\")\n",
    "print_policy_monty_v2(env, policy_off)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
