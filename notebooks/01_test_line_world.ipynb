{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwo_round_rps\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TwoRoundRPS\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrl_algorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalue_iteration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m value_iteration\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrl_algorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_iteration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m policy_iteration\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'envs'"
     ]
    }
   ],
   "source": [
    "from envs.rps.two_round_rps import TwoRoundRPS\n",
    "from rl_algorithms.dp.value_iteration import value_iteration\n",
    "from rl_algorithms.dp.policy_iteration import policy_iteration\n",
    "\n",
    "env = TwoRoundRPS()\n",
    "V, policy = value_iteration(env)\n",
    "policy_pi, V_pi = policy_iteration(env)\n",
    "\n",
    "print(\"\\nValue Iteration: Value Function =\", V)\n",
    "print(\"Value Iteration: Policy =\", policy)\n",
    "print(\"\\nPolicy Iteration: Policy =\", policy_pi)\n",
    "print(\"Policy Iteration: Value Function =\", V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration Dyna-Q LineWorld ===\n",
      "Monde linéaire de taille 7\n",
      "Position de départ: S3\n",
      "États terminaux: S0 (récompense -1) et S6 (récompense +1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DynaQAgent.__init__() got an unexpected keyword argument 'n_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 764\u001b[39m\n\u001b[32m    760\u001b[39m         pygame.quit()  \u001b[38;5;66;03m# Fermer pygame si on ne fait pas le test manuel\u001b[39;00m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 719\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÉtats terminaux: S0 (récompense -1) et S\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_size-\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (récompense +1)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;66;03m# Créer l'agent Dyna-Q\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m agent = \u001b[43mDynaQAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#n_planning=10\u001b[39;49;00m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# ✅ AJOUTÉ\u001b[39;49;00m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplanning_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# ✅ AJOUTÉ\u001b[39;49;00m\n\u001b[32m    727\u001b[39m \n\u001b[32m    728\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent Dyna-Q configuré:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    731\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m- Taux d\u001b[39m\u001b[33m'\u001b[39m\u001b[33mapprentissage (α): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.alpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: DynaQAgent.__init__() got an unexpected keyword argument 'n_states'"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynaQAgent:\n",
    "    \"\"\"Algorithme Dyna-Q selon Sutton & Barto\"\"\"\n",
    "\n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1, planning_steps=5):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.planning_steps = planning_steps\n",
    "\n",
    "        self.Q = np.zeros((n_states, n_actions))  # Table de valeurs Q\n",
    "        self.model = {}                           # Modèle utilisé pour la planification\n",
    "        self.visited_state_actions = set()        # Historique pour la planification\n",
    "    \n",
    "    \"\"\"def __init__(self, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1, n_planning=50):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # Taux d'apprentissage\n",
    "        self.gamma = gamma  # Facteur de discount\n",
    "        self.epsilon = epsilon  # Exploration ε-greedy\n",
    "        self.n_planning = n_planning  # Étapes de planification\n",
    "        \n",
    "        # Table Q : Q[état][action] = valeur\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Modèle : Model[état][action] = (récompense, nouvel_état)\n",
    "        self.Model = defaultdict(lambda: defaultdict(lambda: None))\n",
    "        \n",
    "        # Paires (état, action) visitées pour planification\n",
    "        self.visited_state_actions = set()\n",
    "        self.n_states = n_states          # à ajouter\n",
    "        self.planning_steps = planning_steps\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        self.model = {}  # Pour Dyna-Q\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def select_action(self, state, valid_actions=None):\n",
    "        \"\"\"Politique ε-greedy\"\"\"\n",
    "        if valid_actions is None:\n",
    "            valid_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = [self.Q[state][action] for action in valid_actions]\n",
    "            if not q_values:\n",
    "                return random.choice(valid_actions)\n",
    "            max_q = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def update_q(self, state, action, reward, next_state, valid_next_actions=None):\n",
    "        \"\"\"Mise à jour Q-learning\"\"\"\n",
    "        if valid_next_actions is None:\n",
    "            valid_next_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if valid_next_actions:\n",
    "            max_next_q = max([self.Q[next_state][a] for a in valid_next_actions])\n",
    "        else:\n",
    "            max_next_q = 0.0\n",
    "        \n",
    "        # Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]\n",
    "        current_q = self.Q[state][action]\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "        self.Q[state][action] = current_q + self.alpha * (td_target - current_q)\n",
    "    \n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"Mise à jour du modèle\"\"\"\n",
    "        self.Model[state][action] = (reward, next_state)\n",
    "        self.visited_state_actions.add((state, action))\n",
    "    \n",
    "    def planning_step(self, get_valid_actions_fn=None):\n",
    "        \"\"\"Une étape de planification\"\"\"\n",
    "        if not self.visited_state_actions:\n",
    "            return\n",
    "        \n",
    "        # Échantillonner (état, action) aléatoirement\n",
    "        state, action = random.choice(list(self.visited_state_actions))\n",
    "        \n",
    "        if self.Model[state][action] is not None:\n",
    "            reward, next_state = self.Model[state][action]\n",
    "            \n",
    "            # Actions valides dans next_state\n",
    "            if get_valid_actions_fn:\n",
    "                valid_next_actions = get_valid_actions_fn(next_state)\n",
    "            else:\n",
    "                valid_next_actions = list(range(self.n_actions))\n",
    "            \n",
    "            # Mise à jour Q avec expérience simulée\n",
    "            self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "    \n",
    "    def learn_step(self, state, action, reward, next_state, valid_next_actions=None, get_valid_actions_fn=None):\n",
    "        \"\"\"Étape complète Dyna-Q\"\"\"\n",
    "        \n",
    "        # 1. APPRENTISSAGE DIRECT\n",
    "        self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "        \n",
    "        # 2. MISE À JOUR MODÈLE\n",
    "        self.update_model(state, action, reward, next_state)\n",
    "        \n",
    "        # 3. PLANIFICATION (n étapes)\n",
    "        for _ in range(self.n_planning):\n",
    "            self.planning_step(get_valid_actions_fn)\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\"\n",
    "        Retourne un dictionnaire représentant la politique optimale.\n",
    "        Clé = état (int), valeur = action optimale (0=gauche, 1=droite)\n",
    "        \"\"\"\n",
    "        return {s: int(np.argmax(self.Q[s])) for s in range(self.n_states)}        \n",
    "\n",
    "\n",
    "class LineWorld:\n",
    "    def __init__(self, size=5, start_state=2):\n",
    "        \"\"\"\n",
    "        Initialize the LineWorld environment.\n",
    "\n",
    "        Args:\n",
    "            size (int): Number of states in the environment (default 5).\n",
    "            start_state (int): The agent's initial position (default 2).\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_state = start_state\n",
    "        self.state = start_state\n",
    "        self.terminal_states = [0, size - 1]\n",
    "        self.action_space = [0, 1]  # 0: left, 1: right\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"For compatibility with RL algos. State is already int, so just return.\"\"\"\n",
    "        return state\n",
    "\n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"For compatibility with RL algos. State is already int, so just return.\"\"\"\n",
    "        return index\n",
    "\n",
    "    @property\n",
    "    def n_states(self):\n",
    "        \"\"\"Total number of states (for tabular RL).\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        Returns:\n",
    "            int: The initial state.\n",
    "        \"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Check if a given state is terminal.\n",
    "        Args:\n",
    "            state (int): State to check.\n",
    "        Returns:\n",
    "            bool: True if the state is terminal, False otherwise.\n",
    "        \"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_reward(self, next_state):\n",
    "        \"\"\"\n",
    "        Get the reward for transitioning into the next state.\n",
    "        Args:\n",
    "            next_state (int): The state resulting from the agent's action.\n",
    "        Returns:\n",
    "            float: The reward received for the transition.\n",
    "        \"\"\"\n",
    "        if next_state == 0:\n",
    "            return -1.0\n",
    "        elif next_state == self.size - 1:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def get_valid_actions(self, state):\n",
    "        \"\"\"Get valid actions for a given state\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.action_space.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        Args:\n",
    "            action (int): Action to take (0 = left, 1 = right).\n",
    "        Returns:\n",
    "            tuple: (next_state (int), reward (float), done (bool))\n",
    "        \"\"\"\n",
    "        if self.is_terminal(self.state):\n",
    "            return self.state, 0.0, True\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = max(self.state - 1, 0)\n",
    "        elif action == 1:\n",
    "            next_state = min(self.state + 1, self.size - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action (0=left, 1=right)\")\n",
    "\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_terminal(next_state)\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def simulate_step(self, state, action):\n",
    "        \"\"\"\n",
    "        Simulate a step from a given state and action without modifying self.state.\n",
    "        Returns:\n",
    "            next_state, reward, done\n",
    "        \"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state, 0.0, True\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = max(state - 1, 0)\n",
    "        elif action == 1:\n",
    "            next_state = min(state + 1, self.size - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action (0=left, 1=right)\")\n",
    "\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_terminal(next_state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Print a visual representation of the environment.\n",
    "        'A': Agent position\n",
    "        'T': Terminal state(s)\n",
    "        '_': Normal state\n",
    "        \"\"\"\n",
    "        line = []\n",
    "        for i in range(self.size):\n",
    "            if i == self.state:\n",
    "                line.append('A')\n",
    "            elif i in self.terminal_states:\n",
    "                line.append('T')\n",
    "            else:\n",
    "                line.append('_')\n",
    "        print(' '.join(line))\n",
    "\n",
    "\n",
    "class DynaQLineWorldVisualization:\n",
    "    \"\"\"Visualisation Pygame pour Dyna-Q dans LineWorld\"\"\"\n",
    "    \n",
    "    def __init__(self, world, agent, width=1200, height=700):\n",
    "        pygame.init()\n",
    "        self.world = world\n",
    "        self.agent = agent\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((width, height))\n",
    "        pygame.display.set_caption(\"Dyna-Q LineWorld - Apprentissage par Renforcement\")\n",
    "        \n",
    "        # Couleurs\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.BLUE = (0, 100, 255)\n",
    "        self.RED = (255, 50, 50)\n",
    "        self.GREEN = (50, 255, 50)\n",
    "        self.GRAY = (180, 180, 180)\n",
    "        self.YELLOW = (255, 255, 0)\n",
    "        self.PURPLE = (150, 50, 200)\n",
    "        self.DARK_GREEN = (0, 150, 0)\n",
    "        self.DARK_RED = (150, 0, 0)\n",
    "        self.LIGHT_BLUE = (173, 216, 230)\n",
    "        \n",
    "        # Paramètres de visualisation\n",
    "        self.cell_width = 80\n",
    "        self.cell_height = 80\n",
    "        self.world_y = height // 2 - self.cell_height // 2\n",
    "        self.world_x = (width - world.size * self.cell_width) // 2\n",
    "        \n",
    "        # Polices\n",
    "        self.font_large = pygame.font.Font(None, 28)\n",
    "        self.font_medium = pygame.font.Font(None, 24)\n",
    "        self.font_small = pygame.font.Font(None, 18)\n",
    "        self.font_tiny = pygame.font.Font(None, 14)\n",
    "        \n",
    "        # Statistiques\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.current_episode = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        # Animation\n",
    "        self.animation_step = 0\n",
    "        \n",
    "    def draw_world(self):\n",
    "        \"\"\"Dessiner le monde linéaire\"\"\"\n",
    "        for i in range(self.world.size):\n",
    "            x = self.world_x + i * self.cell_width\n",
    "            y = self.world_y\n",
    "            \n",
    "            # Couleur de base selon le type de cellule\n",
    "            if i in self.world.terminal_states:\n",
    "                if i == 0:  # Terminal négatif\n",
    "                    base_color = self.DARK_RED\n",
    "                    label_color = self.WHITE\n",
    "                    reward_text = \"-1\"\n",
    "                else:  # Terminal positif\n",
    "                    base_color = self.DARK_GREEN\n",
    "                    label_color = self.WHITE\n",
    "                    reward_text = \"+1\"\n",
    "            else:\n",
    "                base_color = self.LIGHT_BLUE\n",
    "                label_color = self.BLACK\n",
    "                reward_text = \"0\"\n",
    "            \n",
    "            # Effet de surbrillance pour la position actuelle\n",
    "            if i == self.world.state:\n",
    "                # Animation de pulsation\n",
    "                pulse = abs(np.sin(self.animation_step * 0.1)) * 30\n",
    "                base_color = tuple(min(255, c + pulse) for c in base_color)\n",
    "            \n",
    "            # Dessiner la cellule\n",
    "            pygame.draw.rect(self.screen, base_color, (x, y, self.cell_width, self.cell_height))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (x, y, self.cell_width, self.cell_height), 3)\n",
    "            \n",
    "            # Numéro de l'état\n",
    "            state_text = self.font_medium.render(f\"S{i}\", True, label_color)\n",
    "            state_rect = state_text.get_rect(center=(x + self.cell_width // 2, y + 15))\n",
    "            self.screen.blit(state_text, state_rect)\n",
    "            \n",
    "            # Récompense\n",
    "            reward_surface = self.font_small.render(f\"R: {reward_text}\", True, label_color)\n",
    "            reward_rect = reward_surface.get_rect(center=(x + self.cell_width // 2, y + 35))\n",
    "            self.screen.blit(reward_surface, reward_rect)\n",
    "            \n",
    "            # Agent (marqueur spécial)\n",
    "            if i == self.world.state:\n",
    "                agent_radius = 15\n",
    "                pygame.draw.circle(self.screen, self.YELLOW, \n",
    "                                 (x + self.cell_width // 2, y + self.cell_height - 25), \n",
    "                                 agent_radius)\n",
    "                pygame.draw.circle(self.screen, self.BLACK, \n",
    "                                 (x + self.cell_width // 2, y + self.cell_height - 25), \n",
    "                                 agent_radius, 2)\n",
    "                agent_text = self.font_small.render(\"A\", True, self.BLACK)\n",
    "                agent_rect = agent_text.get_rect(center=(x + self.cell_width // 2, y + self.cell_height - 25))\n",
    "                self.screen.blit(agent_text, agent_rect)\n",
    "    \n",
    "    def draw_q_values(self):\n",
    "        \"\"\"Dessiner les valeurs Q sous chaque état\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if self.world.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            x = self.world_x + state * self.cell_width\n",
    "            y = self.world_y + self.cell_height + 10\n",
    "            \n",
    "            # Valeurs Q pour cet état\n",
    "            q_left = self.agent.Q[state][0]\n",
    "            q_right = self.agent.Q[state][1]\n",
    "            \n",
    "            # Couleurs basées sur les valeurs\n",
    "            left_color = self.GREEN if q_left > q_right else self.GRAY\n",
    "            right_color = self.GREEN if q_right > q_left else self.GRAY\n",
    "            \n",
    "            # Afficher Q-values avec couleurs\n",
    "            left_text = f\"←{q_left:.2f}\"\n",
    "            right_text = f\"→{q_right:.2f}\"\n",
    "            \n",
    "            left_surface = self.font_tiny.render(left_text, True, left_color)\n",
    "            right_surface = self.font_tiny.render(right_text, True, right_color)\n",
    "            \n",
    "            # Positionner les textes\n",
    "            left_rect = left_surface.get_rect(center=(x + self.cell_width // 4, y))\n",
    "            right_rect = right_surface.get_rect(center=(x + 3 * self.cell_width // 4, y))\n",
    "            \n",
    "            self.screen.blit(left_surface, left_rect)\n",
    "            self.screen.blit(right_surface, right_rect)\n",
    "    \n",
    "    def draw_policy(self):\n",
    "        \"\"\"Dessiner la politique (flèches pour la meilleure action)\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if self.world.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                continue\n",
    "            \n",
    "            # Trouver la meilleure action\n",
    "            q_values = [self.agent.Q[state][action] for action in valid_actions]\n",
    "            if max(q_values) == min(q_values):\n",
    "                continue  # Pas de préférence claire\n",
    "                \n",
    "            best_action = valid_actions[np.argmax(q_values)]\n",
    "            \n",
    "            # Dessiner la flèche\n",
    "            x = self.world_x + state * self.cell_width + self.cell_width // 2\n",
    "            y = self.world_y - 30\n",
    "            \n",
    "            arrow_size = 12\n",
    "            if best_action == 0:  # Gauche\n",
    "                points = [(x-arrow_size, y), (x+arrow_size//2, y-arrow_size//2), (x+arrow_size//2, y+arrow_size//2)]\n",
    "                pygame.draw.polygon(self.screen, self.PURPLE, points)\n",
    "            else:  # Droite\n",
    "                points = [(x+arrow_size, y), (x-arrow_size//2, y-arrow_size//2), (x-arrow_size//2, y+arrow_size//2)]\n",
    "                pygame.draw.polygon(self.screen, self.PURPLE, points)\n",
    "    \n",
    "    def draw_model_info(self):\n",
    "        \"\"\"Afficher les informations sur le modèle appris\"\"\"\n",
    "        y_start = self.world_y + self.cell_height + 60\n",
    "        \n",
    "        title = self.font_medium.render(\"Modèle Dyna-Q (échantillon):\", True, self.BLACK)\n",
    "        self.screen.blit(title, (20, y_start))\n",
    "        \n",
    "        # Afficher quelques transitions apprises\n",
    "        displayed = 0\n",
    "        max_display = 6\n",
    "        \n",
    "        # Parcourir correctement le modèle à deux niveaux\n",
    "        for state in list(self.agent.Model.keys())[:max_display]:\n",
    "            if displayed >= max_display:\n",
    "                break\n",
    "                \n",
    "            for action in self.agent.Model[state]:\n",
    "                if displayed >= max_display:\n",
    "                    break\n",
    "                    \n",
    "                transition = self.agent.Model[state][action]\n",
    "                if transition is not None:  # Vérifier qu'il y a bien une transition\n",
    "                    reward, next_state = transition\n",
    "                    action_name = \"←\" if action == 0 else \"→\"\n",
    "                    text = f\"S{state} {action_name} → S{next_state} (R={reward})\"\n",
    "                    surface = self.font_small.render(text, True, self.BLACK)\n",
    "                    self.screen.blit(surface, (20, y_start + 25 + displayed * 20))\n",
    "                    displayed += 1\n",
    "    \n",
    "    def draw_statistics(self):\n",
    "        \"\"\"Dessiner les statistiques détaillées\"\"\"\n",
    "        stats_x = 20\n",
    "        stats_y = 20\n",
    "        \n",
    "        # Titre\n",
    "        title = self.font_large.render(\"Statistiques Dyna-Q\", True, self.BLACK)\n",
    "        self.screen.blit(title, (stats_x, stats_y))\n",
    "        \n",
    "        # Informations de l'épisode\n",
    "        info_lines = [\n",
    "            f\"Épisode: {self.current_episode}\",\n",
    "            f\"Position: S{self.world.state}\",\n",
    "            f\"Steps totaux: {self.total_steps}\",\n",
    "            \"\",\n",
    "            f\"Paramètres:\",\n",
    "            f\"  ε (exploration): {self.agent.epsilon:.3f}\",\n",
    "            f\"  α (apprentissage): {self.agent.alpha:.3f}\",\n",
    "            f\"  γ (discount): {self.agent.gamma:.3f}\",\n",
    "            f\"  Planning steps: {self.agent.n_planning}\",\n",
    "        ]\n",
    "        \n",
    "        for i, line in enumerate(info_lines):\n",
    "            if line:  # Ignorer les lignes vides pour le rendu\n",
    "                text = self.font_small.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (stats_x, stats_y + 35 + i * 18))\n",
    "        \n",
    "        # Statistiques de performance\n",
    "        if self.episode_rewards:\n",
    "            perf_y = stats_y + 35 + len(info_lines) * 18 + 20\n",
    "            \n",
    "            recent_rewards = self.episode_rewards[-20:]  # 20 derniers épisodes\n",
    "            avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "            \n",
    "            perf_lines = [\n",
    "                \"Performance:\",\n",
    "                f\"  Récompense moyenne (20 derniers): {avg_reward:.3f}\",\n",
    "                f\"  Dernier épisode: {self.episode_rewards[-1]:.3f}\",\n",
    "                f\"  Steps dernier épisode: {self.episode_steps[-1]}\",\n",
    "                f\"  États-actions explorés: {len(self.agent.visited_state_actions)}\",\n",
    "            ]\n",
    "            \n",
    "            for i, line in enumerate(perf_lines):\n",
    "                text = self.font_small.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (stats_x, perf_y + i * 18))\n",
    "    \n",
    "    def draw_legend(self):\n",
    "        \"\"\"Dessiner la légende\"\"\"\n",
    "        legend_x = self.width - 250\n",
    "        legend_y = 20\n",
    "        \n",
    "        title = self.font_medium.render(\"Légende:\", True, self.BLACK)\n",
    "        self.screen.blit(title, (legend_x, legend_y))\n",
    "        \n",
    "        legend_items = [\n",
    "            (\"Agent (A)\", self.YELLOW),\n",
    "            (\"État terminal +1\", self.DARK_GREEN),\n",
    "            (\"État terminal -1\", self.DARK_RED),\n",
    "            (\"État normal\", self.LIGHT_BLUE),\n",
    "            (\"Meilleure action\", self.PURPLE),\n",
    "            (\"Q-value élevée\", self.GREEN),\n",
    "        ]\n",
    "        \n",
    "        for i, (label, color) in enumerate(legend_items):\n",
    "            y = legend_y + 30 + i * 25\n",
    "            pygame.draw.rect(self.screen, color, (legend_x, y, 20, 20))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (legend_x, y, 20, 20), 1)\n",
    "            text = self.font_small.render(label, True, self.BLACK)\n",
    "            self.screen.blit(text, (legend_x + 30, y + 2))\n",
    "    \n",
    "    def draw_progress_bar(self):\n",
    "        \"\"\"Dessiner une barre de progression pour l'épisode\"\"\"\n",
    "        if not hasattr(self, 'episode_length'):\n",
    "            self.episode_length = 100  # Longueur maximale estimée\n",
    "            \n",
    "        bar_x = self.world_x\n",
    "        bar_y = self.world_y + self.cell_height + 120\n",
    "        bar_width = self.world.size * self.cell_width\n",
    "        bar_height = 20\n",
    "        \n",
    "        # Fond de la barre\n",
    "        pygame.draw.rect(self.screen, self.GRAY, (bar_x, bar_y, bar_width, bar_height))\n",
    "        \n",
    "        # Progression\n",
    "        if hasattr(self, 'current_step'):\n",
    "            progress = min(1.0, self.current_step / self.episode_length)\n",
    "            pygame.draw.rect(self.screen, self.GREEN, (bar_x, bar_y, bar_width * progress, bar_height))\n",
    "        \n",
    "        # Bordure\n",
    "        pygame.draw.rect(self.screen, self.BLACK, (bar_x, bar_y, bar_width, bar_height), 2)\n",
    "        \n",
    "        # Texte\n",
    "        text = self.font_small.render(\"Progression de l'épisode\", True, self.BLACK)\n",
    "        text_rect = text.get_rect(center=(bar_x + bar_width // 2, bar_y - 15))\n",
    "        self.screen.blit(text, text_rect)\n",
    "    \n",
    "    def update_display(self):\n",
    "        \"\"\"Mettre à jour l'affichage complet\"\"\"\n",
    "        self.screen.fill(self.WHITE)\n",
    "        \n",
    "        self.draw_world()\n",
    "        self.draw_q_values()\n",
    "        self.draw_policy()\n",
    "        self.draw_statistics()\n",
    "        self.draw_legend()\n",
    "        self.draw_model_info()\n",
    "        self.draw_progress_bar()\n",
    "        \n",
    "        # Incrémenter l'animation\n",
    "        self.animation_step += 1\n",
    "        \n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def run_episode(self, max_steps=100, delay=0.1):\n",
    "        \"\"\"Exécuter un épisode avec visualisation\"\"\"\n",
    "        state = self.world.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        self.current_step = 0\n",
    "        self.episode_length = max_steps\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            self.current_step = step\n",
    "            \n",
    "            # Gérer les événements Pygame\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    return False, total_reward, steps\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        # Pause - attendre une autre pression sur espace\n",
    "                        paused = True\n",
    "                        while paused:\n",
    "                            for pause_event in pygame.event.get():\n",
    "                                if pause_event.type == pygame.QUIT:\n",
    "                                    return False, total_reward, steps\n",
    "                                elif pause_event.type == pygame.KEYDOWN:\n",
    "                                    if pause_event.key == pygame.K_SPACE:\n",
    "                                        paused = False\n",
    "                            time.sleep(0.1)\n",
    "            \n",
    "            # Vérifier si l'état est terminal\n",
    "            if self.world.is_terminal(state):\n",
    "                break\n",
    "            \n",
    "            # Sélectionner et exécuter une action\n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = self.agent.select_action(state, valid_actions)\n",
    "            next_state, reward, done = self.world.step(action)\n",
    "            \n",
    "            # Apprentissage Dyna-Q\n",
    "            valid_next_actions = self.world.get_valid_actions(next_state)\n",
    "            self.agent.learn_step(state, action, reward, next_state, \n",
    "                                valid_next_actions, self.world.get_valid_actions)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            self.total_steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # Mettre à jour l'affichage\n",
    "            self.update_display()\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return True, total_reward, steps\n",
    "    \n",
    "    def run_training(self, n_episodes=200, delay=0.1):\n",
    "        \"\"\"Exécuter l'entraînement complet\"\"\"\n",
    "        print(\"=== Démarrage de l'entraînement Dyna-Q ===\")\n",
    "        print(\"Contrôles:\")\n",
    "        print(\"- ESC: Arrêter l'entraînement\")\n",
    "        print(\"- SPACE: Pause/Reprendre\")\n",
    "        print(\"- Fermer la fenêtre: Quitter\")\n",
    "        print()\n",
    "        \n",
    "        running = True\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            if not running:\n",
    "                break\n",
    "                \n",
    "            self.current_episode = episode + 1\n",
    "            \n",
    "            # Exécuter un épisode\n",
    "            continue_training, reward, steps = self.run_episode(delay=delay)\n",
    "            if not continue_training:\n",
    "                break\n",
    "            \n",
    "            # Enregistrer les statistiques\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_steps.append(steps)\n",
    "            \n",
    "            # Réduire progressivement l'exploration\n",
    "            if episode % 20 == 0 and episode > 0:\n",
    "                self.agent.epsilon = max(0.01, self.agent.epsilon * 0.95)\n",
    "            \n",
    "            # Afficher les progrès dans la console\n",
    "            if episode % 50 == 0 or episode < 10:\n",
    "                recent_rewards = self.episode_rewards[-20:] if len(self.episode_rewards) >= 20 else self.episode_rewards\n",
    "                avg_reward = sum(recent_rewards) / len(recent_rewards) if recent_rewards else 0\n",
    "                print(f\"Épisode {episode}: Récompense moyenne = {avg_reward:.3f}, \"\n",
    "                      f\"Epsilon = {self.agent.epsilon:.3f}, Steps = {steps}\")\n",
    "        \n",
    "        print(\"\\n=== Entraînement terminé ===\")\n",
    "        print(f\"Episodes complétés: {len(self.episode_rewards)}\")\n",
    "        if self.episode_rewards:\n",
    "            final_avg = sum(self.episode_rewards[-50:]) / min(50, len(self.episode_rewards))\n",
    "            print(f\"Performance finale (50 derniers épisodes): {final_avg:.3f}\")\n",
    "\n",
    "        \n",
    "\n",
    "    def run_manual_test(self):\n",
    "        \"\"\"Mode manuel : l'utilisateur contrôle l'agent avec le clavier\"\"\"\n",
    "        print(\"\\n=== MODE TEST MANUEL ===\")\n",
    "        print(\"Touches :\")\n",
    "        print(\"← (flèche gauche) = aller à gauche\")\n",
    "        print(\"→ (flèche droite) = aller à droite\")\n",
    "        print(\"R = Réinitialiser à la position de départ\")\n",
    "        print(\"ESC = Quitter le test\\n\")\n",
    "        \n",
    "        # Réinitialiser l'environnement\n",
    "        state = self.world.reset()\n",
    "        self.current_episode = \"MANUEL\"\n",
    "        self.current_step = 0\n",
    "        self.episode_length = 100\n",
    "        \n",
    "        running = True\n",
    "        clock = pygame.time.Clock()\n",
    "        \n",
    "        # Affichage initial\n",
    "        self.update_display()\n",
    "        \n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    break\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        running = False\n",
    "                        break\n",
    "                    elif event.key == pygame.K_LEFT:\n",
    "                        action = 0\n",
    "                    elif event.key == pygame.K_RIGHT:\n",
    "                        action = 1\n",
    "                    elif event.key == pygame.K_r:\n",
    "                        state = self.world.reset()\n",
    "                        print(\"🔄 Position réinitialisée\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    if action is not None and not self.world.is_terminal(state):\n",
    "                        next_state, reward, done = self.world.step(action)\n",
    "                        print(f\"Action {'←' if action == 0 else '→'} | S{state} → S{next_state} | R = {reward}\")\n",
    "                        state = next_state\n",
    "\n",
    "                        if done:\n",
    "                            print(f\"🎯 État terminal atteint: S{state} | Récompense: {reward}\")\n",
    "                            print(\"Appuie sur R pour recommencer ou ESC pour quitter.\")\n",
    "\n",
    "            # Mettre à jour l'affichage\n",
    "            self.update_display()\n",
    "            clock.tick(30)  # 30 FPS pour une meilleure fluidité\n",
    "        \n",
    "        print(\"Test manuel terminé.\")\n",
    "        # Fermer pygame seulement à la fin\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"=== Configuration Dyna-Q LineWorld ===\")\n",
    "    \n",
    "    # Créer l'environnement LineWorld\n",
    "    world_size = 7\n",
    "    start_pos = world_size // 2\n",
    "    world = LineWorld(size=world_size, start_state=start_pos)\n",
    "    \n",
    "    print(f\"Monde linéaire de taille {world_size}\")\n",
    "    print(f\"Position de départ: S{start_pos}\")\n",
    "    print(f\"États terminaux: S0 (récompense -1) et S{world_size-1} (récompense +1)\")\n",
    "\n",
    "    # Créer l'agent Dyna-Q\n",
    "    agent = DynaQAgent(\n",
    "        n_actions=2,\n",
    "        alpha=0.1,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.3,\n",
    "        #n_planning=10\n",
    "        n_states=world_size,     # ✅ AJOUTÉ\n",
    "        planning_steps=10         # ✅ AJOUTÉ\n",
    "\n",
    "    )\n",
    "\n",
    "    print(f\"Agent Dyna-Q configuré:\")\n",
    "    print(f\"- Taux d'apprentissage (α): {agent.alpha}\")\n",
    "    print(f\"- Facteur de discount (γ): {agent.gamma}\")\n",
    "    print(f\"- Exploration initiale (ε): {agent.epsilon}\")\n",
    "    print(f\"- Étapes de planification: {agent.n_planning}\")\n",
    "    \n",
    "    # Créer la visualisation\n",
    "    viz = DynaQLineWorldVisualization(world, agent)\n",
    "\n",
    "    # Lancer l'entraînement\n",
    "    print(\"\\nLancement de la visualisation...\")\n",
    "    viz.run_training(n_episodes=300, delay=0.08)\n",
    "    \n",
    "    print(\"\\n=== Politique optimale apprise ===\")\n",
    "    optimal_policy = agent.get_optimal_policy()\n",
    "    for state, action in optimal_policy.items():\n",
    "        if state in env.terminal_states:\n",
    "            print(f\"État S{state}: Terminal\")\n",
    "        else:\n",
    "            direction = \"→ droite\" if action == 1 else \"← gauche\"\n",
    "            print(f\"État S{state}: {direction}\")\n",
    "\n",
    "\n",
    "    # Lancer le test manuel après l'entraînement\n",
    "    print(\"\\nSouhaitez-vous tester manuellement le comportement de l'agent ? (y/n)\")\n",
    "    choice = \"y\"  # Force le choix à \"y\"\n",
    "    print(f\"Choix automatique: {choice}\")\n",
    "    if choice == \"y\":\n",
    "        viz.run_manual_test()\n",
    "    else:\n",
    "        pygame.quit()  # Fermer pygame si on ne fait pas le test manuel\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.9.13"
=======
   "version": "3.13.5"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
