{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec1f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Using cached pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp313-cp313-win_amd64.whl.metadata (110 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\school\\s2\\s2 - deep reinforcement learning\\4ia_reinforcementlearning\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\school\\s2\\s2 - deep reinforcement learning\\4ia_reinforcementlearning\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\school\\s2\\s2 - deep reinforcement learning\\4ia_reinforcementlearning\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Using cached numpy-2.3.1-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "Downloading contourpy-1.3.2-cp313-cp313-win_amd64.whl (223 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.1/2.2 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 10.3 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.1/7.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.5/7.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/7.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 9.9 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pygame, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/9 [pyparsing]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   ---- ----------------------------------- 1/9 [pygame]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   -------- ------------------------------- 2/9 [pillow]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   ---------------------- ----------------- 5/9 [fonttools]\n",
      "   -------------------------- ------------- 6/9 [cycler]\n",
      "   ------------------------------- -------- 7/9 [contourpy]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ----------------------------------- ---- 8/9 [matplotlib]\n",
      "   ---------------------------------------- 9/9 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.3 numpy-2.3.1 pillow-11.3.0 pygame-2.6.1 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cbbb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'entraînement Dyna-Q...\n",
      "Appuyez sur ESC pour arrêter, SPACE pour pause\n",
      "Épisode 0: Récompense moyenne = -1.00, Epsilon = 0.285\n",
      "Épisode 20: Récompense moyenne = -1.00, Epsilon = 0.257\n",
      "Épisode 40: Récompense moyenne = -1.00, Epsilon = 0.232\n",
      "Épisode 60: Récompense moyenne = -1.00, Epsilon = 0.210\n",
      "Épisode 80: Récompense moyenne = -1.00, Epsilon = 0.189\n",
      "Épisode 100: Récompense moyenne = -1.00, Epsilon = 0.171\n",
      "Épisode 120: Récompense moyenne = -1.00, Epsilon = 0.154\n",
      "Épisode 140: Récompense moyenne = -1.00, Epsilon = 0.139\n",
      "Épisode 160: Récompense moyenne = -1.00, Epsilon = 0.125\n",
      "Épisode 180: Récompense moyenne = -1.00, Epsilon = 0.113\n",
      "Entraînement terminé!\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import time\n",
    "\n",
    "class DynaQAgent:\n",
    "    \"\"\"Algorithme Dyna-Q selon Sutton & Barto\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1, n_planning=50):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # Taux d'apprentissage\n",
    "        self.gamma = gamma  # Facteur de discount\n",
    "        self.epsilon = epsilon  # Exploration ε-greedy\n",
    "        self.n_planning = n_planning  # Étapes de planification\n",
    "        \n",
    "        # Table Q : Q[état][action] = valeur\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Modèle : Model[état][action] = (récompense, nouvel_état)\n",
    "        self.Model = defaultdict(lambda: defaultdict(lambda: None))\n",
    "        \n",
    "        # Paires (état, action) visitées pour planification\n",
    "        self.visited_state_actions = set()\n",
    "    \n",
    "    def select_action(self, state, valid_actions=None):\n",
    "        \"\"\"Politique ε-greedy\"\"\"\n",
    "        if valid_actions is None:\n",
    "            valid_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = [self.Q[state][action] for action in valid_actions]\n",
    "            max_q = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def update_q(self, state, action, reward, next_state, valid_next_actions=None):\n",
    "        \"\"\"Mise à jour Q-learning\"\"\"\n",
    "        if valid_next_actions is None:\n",
    "            valid_next_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if valid_next_actions:\n",
    "            max_next_q = max([self.Q[next_state][a] for a in valid_next_actions])\n",
    "        else:\n",
    "            max_next_q = 0.0\n",
    "        \n",
    "        # Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]\n",
    "        current_q = self.Q[state][action]\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "        self.Q[state][action] = current_q + self.alpha * (td_target - current_q)\n",
    "    \n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"Mise à jour du modèle\"\"\"\n",
    "        self.Model[state][action] = (reward, next_state)\n",
    "        self.visited_state_actions.add((state, action))\n",
    "    \n",
    "    def planning_step(self, get_valid_actions_fn=None):\n",
    "        \"\"\"Une étape de planification\"\"\"\n",
    "        if not self.visited_state_actions:\n",
    "            return\n",
    "        \n",
    "        # Échantillonner (état, action) aléatoirement\n",
    "        state, action = random.choice(list(self.visited_state_actions))\n",
    "        \n",
    "        if self.Model[state][action] is not None:\n",
    "            reward, next_state = self.Model[state][action]\n",
    "            \n",
    "            # Actions valides dans next_state\n",
    "            if get_valid_actions_fn:\n",
    "                valid_next_actions = get_valid_actions_fn(next_state)\n",
    "            else:\n",
    "                valid_next_actions = list(range(self.n_actions))\n",
    "            \n",
    "            # Mise à jour Q avec expérience simulée\n",
    "            self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "    \n",
    "    def learn_step(self, state, action, reward, next_state, valid_next_actions=None, get_valid_actions_fn=None):\n",
    "        \"\"\"Étape complète Dyna-Q\"\"\"\n",
    "        \n",
    "        # 1. APPRENTISSAGE DIRECT\n",
    "        self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "        \n",
    "        # 2. MISE À JOUR MODÈLE\n",
    "        self.update_model(state, action, reward, next_state)\n",
    "        \n",
    "        # 3. PLANIFICATION (n étapes)\n",
    "        for _ in range(self.n_planning):\n",
    "            self.planning_step(get_valid_actions_fn)\n",
    "\n",
    "class LineWorld:\n",
    "    \"\"\"Environnement Line World - monde linéaire 1D\"\"\"\n",
    "    \n",
    "    def __init__(self, size=10, goal_state=None, obstacles=None):\n",
    "        self.size = size\n",
    "        self.goal_state = goal_state if goal_state is not None else size - 1\n",
    "        self.obstacles = obstacles if obstacles is not None else []\n",
    "        self.current_state = 0\n",
    "        self.actions = ['LEFT', 'RIGHT']  # 0: gauche, 1: droite\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Réinitialiser l'environnement\"\"\"\n",
    "        self.current_state = 0\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        \"\"\"Retourne les actions valides pour un état donné\"\"\"\n",
    "        valid_actions = []\n",
    "        \n",
    "        # Gauche (0) : possible si pas au début et pas d'obstacle\n",
    "        if state > 0 and (state - 1) not in self.obstacles:\n",
    "            valid_actions.append(0)\n",
    "            \n",
    "        # Droite (1) : possible si pas à la fin et pas d'obstacle\n",
    "        if state < self.size - 1 and (state + 1) not in self.obstacles:\n",
    "            valid_actions.append(1)\n",
    "            \n",
    "        return valid_actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Exécuter une action\"\"\"\n",
    "        valid_actions = self.get_valid_actions(self.current_state)\n",
    "        \n",
    "        if action not in valid_actions:\n",
    "            # Action invalide, reste sur place\n",
    "            reward = -0.1  # Petite pénalité\n",
    "            done = False\n",
    "            return self.current_state, reward, done\n",
    "        \n",
    "        # Exécuter l'action\n",
    "        if action == 0:  # Gauche\n",
    "            self.current_state = max(0, self.current_state - 1)\n",
    "        elif action == 1:  # Droite\n",
    "            self.current_state = min(self.size - 1, self.current_state + 1)\n",
    "        \n",
    "        # Calculer la récompense\n",
    "        if self.current_state == self.goal_state:\n",
    "            reward = 10.0  # Récompense pour atteindre le but\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.01  # Petite pénalité pour chaque pas\n",
    "            done = False\n",
    "            \n",
    "        return self.current_state, reward, done\n",
    "\n",
    "class DynaQVisualization:\n",
    "    \"\"\"Visualisation Pygame pour Dyna-Q dans Line World\"\"\"\n",
    "    \n",
    "    def __init__(self, world, agent, width=1200, height=600):\n",
    "        pygame.init()\n",
    "        self.world = world\n",
    "        self.agent = agent\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((width, height))\n",
    "        pygame.display.set_caption(\"Dyna-Q Line World\")\n",
    "        \n",
    "        # Couleurs\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.GRAY = (128, 128, 128)\n",
    "        self.YELLOW = (255, 255, 0)\n",
    "        self.PURPLE = (128, 0, 128)\n",
    "        \n",
    "        # Paramètres de visualisation\n",
    "        self.cell_width = 60\n",
    "        self.cell_height = 60\n",
    "        self.world_y = height // 2 - self.cell_height // 2\n",
    "        self.world_x = (width - world.size * self.cell_width) // 2\n",
    "        \n",
    "        self.font = pygame.font.Font(None, 24)\n",
    "        self.small_font = pygame.font.Font(None, 16)\n",
    "        \n",
    "        # Statistiques\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.current_episode = 0\n",
    "        \n",
    "    def draw_world(self):\n",
    "        \"\"\"Dessiner le monde linéaire\"\"\"\n",
    "        for i in range(self.world.size):\n",
    "            x = self.world_x + i * self.cell_width\n",
    "            y = self.world_y\n",
    "            \n",
    "            # Couleur de la cellule\n",
    "            if i == self.world.goal_state:\n",
    "                color = self.GREEN\n",
    "            elif i in self.world.obstacles:\n",
    "                color = self.GRAY\n",
    "            elif i == self.world.current_state:\n",
    "                color = self.BLUE\n",
    "            else:\n",
    "                color = self.WHITE\n",
    "            \n",
    "            # Dessiner la cellule\n",
    "            pygame.draw.rect(self.screen, color, (x, y, self.cell_width, self.cell_height))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (x, y, self.cell_width, self.cell_height), 2)\n",
    "            \n",
    "            # Numéro de l'état\n",
    "            text = self.font.render(str(i), True, self.BLACK)\n",
    "            text_rect = text.get_rect(center=(x + self.cell_width // 2, y + self.cell_height // 2))\n",
    "            self.screen.blit(text, text_rect)\n",
    "    \n",
    "    def draw_q_values(self):\n",
    "        \"\"\"Dessiner les valeurs Q sous chaque état\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            x = self.world_x + state * self.cell_width\n",
    "            y = self.world_y + self.cell_height + 10\n",
    "            \n",
    "            # Valeurs Q pour cet état\n",
    "            q_left = self.agent.Q[state][0]\n",
    "            q_right = self.agent.Q[state][1]\n",
    "            \n",
    "            # Afficher Q-values\n",
    "            q_text = f\"L:{q_left:.2f} R:{q_right:.2f}\"\n",
    "            text = self.small_font.render(q_text, True, self.BLACK)\n",
    "            text_rect = text.get_rect(center=(x + self.cell_width // 2, y))\n",
    "            self.screen.blit(text, text_rect)\n",
    "    \n",
    "    def draw_policy(self):\n",
    "        \"\"\"Dessiner la politique (flèches)\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if state == self.world.goal_state:\n",
    "                continue\n",
    "                \n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                continue\n",
    "                \n",
    "            # Choisir la meilleure action\n",
    "            q_values = [self.agent.Q[state][action] for action in valid_actions]\n",
    "            if max(q_values) == min(q_values):\n",
    "                continue  # Pas de préférence claire\n",
    "                \n",
    "            best_action = valid_actions[np.argmax(q_values)]\n",
    "            \n",
    "            # Dessiner la flèche\n",
    "            x = self.world_x + state * self.cell_width + self.cell_width // 2\n",
    "            y = self.world_y - 20\n",
    "            \n",
    "            if best_action == 0:  # Gauche\n",
    "                pygame.draw.polygon(self.screen, self.RED, \n",
    "                                  [(x-10, y), (x+5, y-5), (x+5, y+5)])\n",
    "            else:  # Droite\n",
    "                pygame.draw.polygon(self.screen, self.RED, \n",
    "                                  [(x+10, y), (x-5, y-5), (x-5, y+5)])\n",
    "    \n",
    "    def draw_stats(self):\n",
    "        \"\"\"Dessiner les statistiques\"\"\"\n",
    "        y_offset = 20\n",
    "        \n",
    "        # Informations générales\n",
    "        info_lines = [\n",
    "            f\"Épisode: {self.current_episode}\",\n",
    "            f\"Position: {self.world.current_state}\",\n",
    "            f\"Epsilon: {self.agent.epsilon:.3f}\",\n",
    "            f\"Alpha: {self.agent.alpha:.3f}\",\n",
    "            f\"Planning steps: {self.agent.n_planning}\",\n",
    "        ]\n",
    "        \n",
    "        for i, line in enumerate(info_lines):\n",
    "            text = self.font.render(line, True, self.BLACK)\n",
    "            self.screen.blit(text, (20, y_offset + i * 25))\n",
    "        \n",
    "        # Statistiques récentes\n",
    "        if self.episode_rewards:\n",
    "            recent_rewards = self.episode_rewards[-10:]  # 10 derniers épisodes\n",
    "            avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "            \n",
    "            stats_lines = [\n",
    "                f\"Récompense moyenne (10 derniers): {avg_reward:.2f}\",\n",
    "                f\"Dernier épisode - Récompense: {self.episode_rewards[-1]:.2f}\",\n",
    "                f\"Dernier épisode - Steps: {self.episode_steps[-1]}\",\n",
    "            ]\n",
    "            \n",
    "            for i, line in enumerate(stats_lines):\n",
    "                text = self.font.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (20, y_offset + 150 + i * 25))\n",
    "    \n",
    "    def draw_legend(self):\n",
    "        \"\"\"Dessiner la légende\"\"\"\n",
    "        legend_x = self.width - 200\n",
    "        legend_y = 20\n",
    "        \n",
    "        legend_items = [\n",
    "            (\"Agent\", self.BLUE),\n",
    "            (\"But\", self.GREEN),\n",
    "            (\"Obstacle\", self.GRAY),\n",
    "            (\"Vide\", self.WHITE),\n",
    "        ]\n",
    "        \n",
    "        title = self.font.render(\"Légende:\", True, self.BLACK)\n",
    "        self.screen.blit(title, (legend_x, legend_y))\n",
    "        \n",
    "        for i, (label, color) in enumerate(legend_items):\n",
    "            y = legend_y + 30 + i * 25\n",
    "            pygame.draw.rect(self.screen, color, (legend_x, y, 20, 20))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (legend_x, y, 20, 20), 1)\n",
    "            text = self.font.render(label, True, self.BLACK)\n",
    "            self.screen.blit(text, (legend_x + 30, y))\n",
    "    \n",
    "    def update_display(self):\n",
    "        \"\"\"Mettre à jour l'affichage\"\"\"\n",
    "        self.screen.fill(self.WHITE)\n",
    "        \n",
    "        self.draw_world()\n",
    "        self.draw_q_values()\n",
    "        self.draw_policy()\n",
    "        self.draw_stats()\n",
    "        self.draw_legend()\n",
    "        \n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def run_episode(self, max_steps=100, delay=0.1):\n",
    "        \"\"\"Exécuter un épisode avec visualisation\"\"\"\n",
    "        state = self.world.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Gérer les événements Pygame\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    return False, total_reward, steps\n",
    "            \n",
    "            # Sélectionner et exécuter une action\n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = self.agent.select_action(state, valid_actions)\n",
    "            next_state, reward, done = self.world.step(action)\n",
    "            \n",
    "            # Apprentissage Dyna-Q\n",
    "            valid_next_actions = self.world.get_valid_actions(next_state)\n",
    "            self.agent.learn_step(state, action, reward, next_state, \n",
    "                                valid_next_actions, self.world.get_valid_actions)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # Mettre à jour l'affichage\n",
    "            self.update_display()\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return True, total_reward, steps\n",
    "    \n",
    "    def run_training(self, n_episodes=100, delay=0.1):\n",
    "        \"\"\"Exécuter l'entraînement complet\"\"\"\n",
    "        print(\"Démarrage de l'entraînement Dyna-Q...\")\n",
    "        print(\"Appuyez sur ESC pour arrêter, SPACE pour pause\")\n",
    "        \n",
    "        running = True\n",
    "        paused = False\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            if not running:\n",
    "                break\n",
    "                \n",
    "            self.current_episode = episode + 1\n",
    "            \n",
    "            # Gérer les événements\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    break\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        running = False\n",
    "                        break\n",
    "                    elif event.key == pygame.K_SPACE:\n",
    "                        paused = not paused\n",
    "            \n",
    "            if paused:\n",
    "                continue\n",
    "            \n",
    "            # Exécuter un épisode\n",
    "            continue_training, reward, steps = self.run_episode(delay=delay)\n",
    "            if not continue_training:\n",
    "                break\n",
    "            \n",
    "            # Enregistrer les statistiques\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_steps.append(steps)\n",
    "            \n",
    "            # Réduire progressivement l'exploration\n",
    "            if episode % 10 == 0:\n",
    "                self.agent.epsilon = max(0.01, self.agent.epsilon * 0.95)\n",
    "            \n",
    "            # Afficher les progrès\n",
    "            if episode % 20 == 0:\n",
    "                avg_reward = sum(self.episode_rewards[-20:]) / min(20, len(self.episode_rewards))\n",
    "                print(f\"Épisode {episode}: Récompense moyenne = {avg_reward:.2f}, \"\n",
    "                      f\"Epsilon = {self.agent.epsilon:.3f}\")\n",
    "        \n",
    "        pygame.quit()\n",
    "        print(\"Entraînement terminé!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    # Créer l'environnement Line World\n",
    "    world = LineWorld(size=8, goal_state=7, obstacles=[3, 5])\n",
    "    \n",
    "    # Créer l'agent Dyna-Q\n",
    "    agent = DynaQAgent(\n",
    "        n_actions=2,\n",
    "        alpha=0.1,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.3,\n",
    "        n_planning=10\n",
    "    )\n",
    "    \n",
    "    # Créer la visualisation\n",
    "    viz = DynaQVisualization(world, agent)\n",
    "    \n",
    "    # Lancer l'entraînement\n",
    "    viz.run_training(n_episodes=200, delay=0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "=== Configuration Dyna-Q LineWorld ===\n",
      "Monde linéaire de taille 7\n",
      "Position de départ: S3\n",
      "États terminaux: S0 (récompense -1) et S6 (récompense +1)\n",
      "Agent Dyna-Q configuré:\n",
      "- Taux d'apprentissage (α): 0.1\n",
      "- Facteur de discount (γ): 0.95\n",
      "- Exploration initiale (ε): 0.3\n",
      "- Étapes de planification: 10\n",
      "\n",
      "Lancement de la visualisation...\n",
      "=== Démarrage de l'entraînement Dyna-Q ===\n",
      "Contrôles:\n",
      "- ESC: Arrêter l'entraînement\n",
      "- SPACE: Pause/Reprendre\n",
      "- Fermer la fenêtre: Quitter\n",
      "\n",
      "Épisode 0: Récompense moyenne = -1.000, Epsilon = 0.300, Steps = 13\n",
      "Épisode 1: Récompense moyenne = 0.000, Epsilon = 0.300, Steps = 3\n",
      "Épisode 2: Récompense moyenne = 0.333, Epsilon = 0.300, Steps = 7\n",
      "Épisode 3: Récompense moyenne = 0.500, Epsilon = 0.300, Steps = 5\n",
      "Épisode 4: Récompense moyenne = 0.600, Epsilon = 0.300, Steps = 3\n",
      "Épisode 5: Récompense moyenne = 0.667, Epsilon = 0.300, Steps = 9\n",
      "Épisode 6: Récompense moyenne = 0.714, Epsilon = 0.300, Steps = 5\n",
      "Épisode 7: Récompense moyenne = 0.750, Epsilon = 0.300, Steps = 3\n",
      "Épisode 8: Récompense moyenne = 0.778, Epsilon = 0.300, Steps = 5\n",
      "Épisode 9: Récompense moyenne = 0.800, Epsilon = 0.300, Steps = 3\n",
      "Épisode 50: Récompense moyenne = 1.000, Epsilon = 0.271, Steps = 3\n",
      "Épisode 100: Récompense moyenne = 1.000, Epsilon = 0.232, Steps = 3\n",
      "Épisode 150: Récompense moyenne = 1.000, Epsilon = 0.210, Steps = 3\n",
      "Épisode 200: Récompense moyenne = 1.000, Epsilon = 0.180, Steps = 3\n",
      "Épisode 250: Récompense moyenne = 1.000, Epsilon = 0.162, Steps = 3\n",
      "\n",
      "=== Entraînement terminé ===\n",
      "Episodes complétés: 300\n",
      "Performance finale (50 derniers épisodes): 1.000\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynaQAgent:\n",
    "    \"\"\"Algorithme Dyna-Q selon Sutton & Barto\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1, n_planning=50):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # Taux d'apprentissage\n",
    "        self.gamma = gamma  # Facteur de discount\n",
    "        self.epsilon = epsilon  # Exploration ε-greedy\n",
    "        self.n_planning = n_planning  # Étapes de planification\n",
    "        \n",
    "        # Table Q : Q[état][action] = valeur\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Modèle : Model[état][action] = (récompense, nouvel_état)\n",
    "        self.Model = defaultdict(lambda: defaultdict(lambda: None))\n",
    "        \n",
    "        # Paires (état, action) visitées pour planification\n",
    "        self.visited_state_actions = set()\n",
    "    \n",
    "    def select_action(self, state, valid_actions=None):\n",
    "        \"\"\"Politique ε-greedy\"\"\"\n",
    "        if valid_actions is None:\n",
    "            valid_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = [self.Q[state][action] for action in valid_actions]\n",
    "            if not q_values:\n",
    "                return random.choice(valid_actions)\n",
    "            max_q = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def update_q(self, state, action, reward, next_state, valid_next_actions=None):\n",
    "        \"\"\"Mise à jour Q-learning\"\"\"\n",
    "        if valid_next_actions is None:\n",
    "            valid_next_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if valid_next_actions:\n",
    "            max_next_q = max([self.Q[next_state][a] for a in valid_next_actions])\n",
    "        else:\n",
    "            max_next_q = 0.0\n",
    "        \n",
    "        # Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]\n",
    "        current_q = self.Q[state][action]\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "        self.Q[state][action] = current_q + self.alpha * (td_target - current_q)\n",
    "    \n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"Mise à jour du modèle\"\"\"\n",
    "        self.Model[state][action] = (reward, next_state)\n",
    "        self.visited_state_actions.add((state, action))\n",
    "    \n",
    "    def planning_step(self, get_valid_actions_fn=None):\n",
    "        \"\"\"Une étape de planification\"\"\"\n",
    "        if not self.visited_state_actions:\n",
    "            return\n",
    "        \n",
    "        # Échantillonner (état, action) aléatoirement\n",
    "        state, action = random.choice(list(self.visited_state_actions))\n",
    "        \n",
    "        if self.Model[state][action] is not None:\n",
    "            reward, next_state = self.Model[state][action]\n",
    "            \n",
    "            # Actions valides dans next_state\n",
    "            if get_valid_actions_fn:\n",
    "                valid_next_actions = get_valid_actions_fn(next_state)\n",
    "            else:\n",
    "                valid_next_actions = list(range(self.n_actions))\n",
    "            \n",
    "            # Mise à jour Q avec expérience simulée\n",
    "            self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "    \n",
    "    def learn_step(self, state, action, reward, next_state, valid_next_actions=None, get_valid_actions_fn=None):\n",
    "        \"\"\"Étape complète Dyna-Q\"\"\"\n",
    "        \n",
    "        # 1. APPRENTISSAGE DIRECT\n",
    "        self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "        \n",
    "        # 2. MISE À JOUR MODÈLE\n",
    "        self.update_model(state, action, reward, next_state)\n",
    "        \n",
    "        # 3. PLANIFICATION (n étapes)\n",
    "        for _ in range(self.n_planning):\n",
    "            self.planning_step(get_valid_actions_fn)\n",
    "\n",
    "\n",
    "class LineWorld:\n",
    "    def __init__(self, size=5, start_state=2):\n",
    "        \"\"\"\n",
    "        Initialize the LineWorld environment.\n",
    "\n",
    "        Args:\n",
    "            size (int): Number of states in the environment (default 5).\n",
    "            start_state (int): The agent's initial position (default 2).\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_state = start_state\n",
    "        self.state = start_state\n",
    "        self.terminal_states = [0, size - 1]\n",
    "        self.action_space = [0, 1]  # 0: left, 1: right\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"For compatibility with RL algos. State is already int, so just return.\"\"\"\n",
    "        return state\n",
    "\n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"For compatibility with RL algos. State is already int, so just return.\"\"\"\n",
    "        return index\n",
    "\n",
    "    @property\n",
    "    def n_states(self):\n",
    "        \"\"\"Total number of states (for tabular RL).\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        Returns:\n",
    "            int: The initial state.\n",
    "        \"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Check if a given state is terminal.\n",
    "        Args:\n",
    "            state (int): State to check.\n",
    "        Returns:\n",
    "            bool: True if the state is terminal, False otherwise.\n",
    "        \"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_reward(self, next_state):\n",
    "        \"\"\"\n",
    "        Get the reward for transitioning into the next state.\n",
    "        Args:\n",
    "            next_state (int): The state resulting from the agent's action.\n",
    "        Returns:\n",
    "            float: The reward received for the transition.\n",
    "        \"\"\"\n",
    "        if next_state == 0:\n",
    "            return -1.0\n",
    "        elif next_state == self.size - 1:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def get_valid_actions(self, state):\n",
    "        \"\"\"Get valid actions for a given state\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.action_space.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        Args:\n",
    "            action (int): Action to take (0 = left, 1 = right).\n",
    "        Returns:\n",
    "            tuple: (next_state (int), reward (float), done (bool))\n",
    "        \"\"\"\n",
    "        if self.is_terminal(self.state):\n",
    "            return self.state, 0.0, True\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = max(self.state - 1, 0)\n",
    "        elif action == 1:\n",
    "            next_state = min(self.state + 1, self.size - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action (0=left, 1=right)\")\n",
    "\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_terminal(next_state)\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def simulate_step(self, state, action):\n",
    "        \"\"\"\n",
    "        Simulate a step from a given state and action without modifying self.state.\n",
    "        Returns:\n",
    "            next_state, reward, done\n",
    "        \"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state, 0.0, True\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = max(state - 1, 0)\n",
    "        elif action == 1:\n",
    "            next_state = min(state + 1, self.size - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action (0=left, 1=right)\")\n",
    "\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_terminal(next_state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Print a visual representation of the environment.\n",
    "        'A': Agent position\n",
    "        'T': Terminal state(s)\n",
    "        '_': Normal state\n",
    "        \"\"\"\n",
    "        line = []\n",
    "        for i in range(self.size):\n",
    "            if i == self.state:\n",
    "                line.append('A')\n",
    "            elif i in self.terminal_states:\n",
    "                line.append('T')\n",
    "            else:\n",
    "                line.append('_')\n",
    "        print(' '.join(line))\n",
    "\n",
    "\n",
    "class DynaQLineWorldVisualization:\n",
    "    \"\"\"Visualisation Pygame pour Dyna-Q dans LineWorld\"\"\"\n",
    "    \n",
    "    def __init__(self, world, agent, width=1200, height=700):\n",
    "        pygame.init()\n",
    "        self.world = world\n",
    "        self.agent = agent\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((width, height))\n",
    "        pygame.display.set_caption(\"Dyna-Q LineWorld - Apprentissage par Renforcement\")\n",
    "        \n",
    "        # Couleurs\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.BLUE = (0, 100, 255)\n",
    "        self.RED = (255, 50, 50)\n",
    "        self.GREEN = (50, 255, 50)\n",
    "        self.GRAY = (180, 180, 180)\n",
    "        self.YELLOW = (255, 255, 0)\n",
    "        self.PURPLE = (150, 50, 200)\n",
    "        self.DARK_GREEN = (0, 150, 0)\n",
    "        self.DARK_RED = (150, 0, 0)\n",
    "        self.LIGHT_BLUE = (173, 216, 230)\n",
    "        \n",
    "        # Paramètres de visualisation\n",
    "        self.cell_width = 80\n",
    "        self.cell_height = 80\n",
    "        self.world_y = height // 2 - self.cell_height // 2\n",
    "        self.world_x = (width - world.size * self.cell_width) // 2\n",
    "        \n",
    "        # Polices\n",
    "        self.font_large = pygame.font.Font(None, 28)\n",
    "        self.font_medium = pygame.font.Font(None, 24)\n",
    "        self.font_small = pygame.font.Font(None, 18)\n",
    "        self.font_tiny = pygame.font.Font(None, 14)\n",
    "        \n",
    "        # Statistiques\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.current_episode = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        # Animation\n",
    "        self.animation_step = 0\n",
    "        \n",
    "    def draw_world(self):\n",
    "        \"\"\"Dessiner le monde linéaire\"\"\"\n",
    "        for i in range(self.world.size):\n",
    "            x = self.world_x + i * self.cell_width\n",
    "            y = self.world_y\n",
    "            \n",
    "            # Couleur de base selon le type de cellule\n",
    "            if i in self.world.terminal_states:\n",
    "                if i == 0:  # Terminal négatif\n",
    "                    base_color = self.DARK_RED\n",
    "                    label_color = self.WHITE\n",
    "                    reward_text = \"-1\"\n",
    "                else:  # Terminal positif\n",
    "                    base_color = self.DARK_GREEN\n",
    "                    label_color = self.WHITE\n",
    "                    reward_text = \"+1\"\n",
    "            else:\n",
    "                base_color = self.LIGHT_BLUE\n",
    "                label_color = self.BLACK\n",
    "                reward_text = \"0\"\n",
    "            \n",
    "            # Effet de surbrillance pour la position actuelle\n",
    "            if i == self.world.state:\n",
    "                # Animation de pulsation\n",
    "                pulse = abs(np.sin(self.animation_step * 0.1)) * 30\n",
    "                base_color = tuple(min(255, c + pulse) for c in base_color)\n",
    "            \n",
    "            # Dessiner la cellule\n",
    "            pygame.draw.rect(self.screen, base_color, (x, y, self.cell_width, self.cell_height))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (x, y, self.cell_width, self.cell_height), 3)\n",
    "            \n",
    "            # Numéro de l'état\n",
    "            state_text = self.font_medium.render(f\"S{i}\", True, label_color)\n",
    "            state_rect = state_text.get_rect(center=(x + self.cell_width // 2, y + 15))\n",
    "            self.screen.blit(state_text, state_rect)\n",
    "            \n",
    "            # Récompense\n",
    "            reward_surface = self.font_small.render(f\"R: {reward_text}\", True, label_color)\n",
    "            reward_rect = reward_surface.get_rect(center=(x + self.cell_width // 2, y + 35))\n",
    "            self.screen.blit(reward_surface, reward_rect)\n",
    "            \n",
    "            # Agent (marqueur spécial)\n",
    "            if i == self.world.state:\n",
    "                agent_radius = 15\n",
    "                pygame.draw.circle(self.screen, self.YELLOW, \n",
    "                                 (x + self.cell_width // 2, y + self.cell_height - 25), \n",
    "                                 agent_radius)\n",
    "                pygame.draw.circle(self.screen, self.BLACK, \n",
    "                                 (x + self.cell_width // 2, y + self.cell_height - 25), \n",
    "                                 agent_radius, 2)\n",
    "                agent_text = self.font_small.render(\"A\", True, self.BLACK)\n",
    "                agent_rect = agent_text.get_rect(center=(x + self.cell_width // 2, y + self.cell_height - 25))\n",
    "                self.screen.blit(agent_text, agent_rect)\n",
    "    \n",
    "    def draw_q_values(self):\n",
    "        \"\"\"Dessiner les valeurs Q sous chaque état\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if self.world.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            x = self.world_x + state * self.cell_width\n",
    "            y = self.world_y + self.cell_height + 10\n",
    "            \n",
    "            # Valeurs Q pour cet état\n",
    "            q_left = self.agent.Q[state][0]\n",
    "            q_right = self.agent.Q[state][1]\n",
    "            \n",
    "            # Couleurs basées sur les valeurs\n",
    "            left_color = self.GREEN if q_left > q_right else self.GRAY\n",
    "            right_color = self.GREEN if q_right > q_left else self.GRAY\n",
    "            \n",
    "            # Afficher Q-values avec couleurs\n",
    "            left_text = f\"←{q_left:.2f}\"\n",
    "            right_text = f\"→{q_right:.2f}\"\n",
    "            \n",
    "            left_surface = self.font_tiny.render(left_text, True, left_color)\n",
    "            right_surface = self.font_tiny.render(right_text, True, right_color)\n",
    "            \n",
    "            # Positionner les textes\n",
    "            left_rect = left_surface.get_rect(center=(x + self.cell_width // 4, y))\n",
    "            right_rect = right_surface.get_rect(center=(x + 3 * self.cell_width // 4, y))\n",
    "            \n",
    "            self.screen.blit(left_surface, left_rect)\n",
    "            self.screen.blit(right_surface, right_rect)\n",
    "    \n",
    "    def draw_policy(self):\n",
    "        \"\"\"Dessiner la politique (flèches pour la meilleure action)\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if self.world.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                continue\n",
    "            \n",
    "            # Trouver la meilleure action\n",
    "            q_values = [self.agent.Q[state][action] for action in valid_actions]\n",
    "            if max(q_values) == min(q_values):\n",
    "                continue  # Pas de préférence claire\n",
    "                \n",
    "            best_action = valid_actions[np.argmax(q_values)]\n",
    "            \n",
    "            # Dessiner la flèche\n",
    "            x = self.world_x + state * self.cell_width + self.cell_width // 2\n",
    "            y = self.world_y - 30\n",
    "            \n",
    "            arrow_size = 12\n",
    "            if best_action == 0:  # Gauche\n",
    "                points = [(x-arrow_size, y), (x+arrow_size//2, y-arrow_size//2), (x+arrow_size//2, y+arrow_size//2)]\n",
    "                pygame.draw.polygon(self.screen, self.PURPLE, points)\n",
    "            else:  # Droite\n",
    "                points = [(x+arrow_size, y), (x-arrow_size//2, y-arrow_size//2), (x-arrow_size//2, y+arrow_size//2)]\n",
    "                pygame.draw.polygon(self.screen, self.PURPLE, points)\n",
    "    \n",
    "    def draw_model_info(self):\n",
    "        \"\"\"Afficher les informations sur le modèle appris\"\"\"\n",
    "        y_start = self.world_y + self.cell_height + 60\n",
    "        \n",
    "        title = self.font_medium.render(\"Modèle Dyna-Q (échantillon):\", True, self.BLACK)\n",
    "        self.screen.blit(title, (20, y_start))\n",
    "        \n",
    "        # Afficher quelques transitions apprises\n",
    "        displayed = 0\n",
    "        max_display = 6\n",
    "        \n",
    "        # Parcourir correctement le modèle à deux niveaux\n",
    "        for state in list(self.agent.Model.keys())[:max_display]:\n",
    "            if displayed >= max_display:\n",
    "                break\n",
    "                \n",
    "            for action in self.agent.Model[state]:\n",
    "                if displayed >= max_display:\n",
    "                    break\n",
    "                    \n",
    "                transition = self.agent.Model[state][action]\n",
    "                if transition is not None:  # Vérifier qu'il y a bien une transition\n",
    "                    reward, next_state = transition\n",
    "                    action_name = \"←\" if action == 0 else \"→\"\n",
    "                    text = f\"S{state} {action_name} → S{next_state} (R={reward})\"\n",
    "                    surface = self.font_small.render(text, True, self.BLACK)\n",
    "                    self.screen.blit(surface, (20, y_start + 25 + displayed * 20))\n",
    "                    displayed += 1\n",
    "    \n",
    "    def draw_statistics(self):\n",
    "        \"\"\"Dessiner les statistiques détaillées\"\"\"\n",
    "        stats_x = 20\n",
    "        stats_y = 20\n",
    "        \n",
    "        # Titre\n",
    "        title = self.font_large.render(\"Statistiques Dyna-Q\", True, self.BLACK)\n",
    "        self.screen.blit(title, (stats_x, stats_y))\n",
    "        \n",
    "        # Informations de l'épisode\n",
    "        info_lines = [\n",
    "            f\"Épisode: {self.current_episode}\",\n",
    "            f\"Position: S{self.world.state}\",\n",
    "            f\"Steps totaux: {self.total_steps}\",\n",
    "            \"\",\n",
    "            f\"Paramètres:\",\n",
    "            f\"  ε (exploration): {self.agent.epsilon:.3f}\",\n",
    "            f\"  α (apprentissage): {self.agent.alpha:.3f}\",\n",
    "            f\"  γ (discount): {self.agent.gamma:.3f}\",\n",
    "            f\"  Planning steps: {self.agent.n_planning}\",\n",
    "        ]\n",
    "        \n",
    "        for i, line in enumerate(info_lines):\n",
    "            if line:  # Ignorer les lignes vides pour le rendu\n",
    "                text = self.font_small.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (stats_x, stats_y + 35 + i * 18))\n",
    "        \n",
    "        # Statistiques de performance\n",
    "        if self.episode_rewards:\n",
    "            perf_y = stats_y + 35 + len(info_lines) * 18 + 20\n",
    "            \n",
    "            recent_rewards = self.episode_rewards[-20:]  # 20 derniers épisodes\n",
    "            avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "            \n",
    "            perf_lines = [\n",
    "                \"Performance:\",\n",
    "                f\"  Récompense moyenne (20 derniers): {avg_reward:.3f}\",\n",
    "                f\"  Dernier épisode: {self.episode_rewards[-1]:.3f}\",\n",
    "                f\"  Steps dernier épisode: {self.episode_steps[-1]}\",\n",
    "                f\"  États-actions explorés: {len(self.agent.visited_state_actions)}\",\n",
    "            ]\n",
    "            \n",
    "            for i, line in enumerate(perf_lines):\n",
    "                text = self.font_small.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (stats_x, perf_y + i * 18))\n",
    "    \n",
    "    def draw_legend(self):\n",
    "        \"\"\"Dessiner la légende\"\"\"\n",
    "        legend_x = self.width - 250\n",
    "        legend_y = 20\n",
    "        \n",
    "        title = self.font_medium.render(\"Légende:\", True, self.BLACK)\n",
    "        self.screen.blit(title, (legend_x, legend_y))\n",
    "        \n",
    "        legend_items = [\n",
    "            (\"Agent (A)\", self.YELLOW),\n",
    "            (\"État terminal +1\", self.DARK_GREEN),\n",
    "            (\"État terminal -1\", self.DARK_RED),\n",
    "            (\"État normal\", self.LIGHT_BLUE),\n",
    "            (\"Meilleure action\", self.PURPLE),\n",
    "            (\"Q-value élevée\", self.GREEN),\n",
    "        ]\n",
    "        \n",
    "        for i, (label, color) in enumerate(legend_items):\n",
    "            y = legend_y + 30 + i * 25\n",
    "            pygame.draw.rect(self.screen, color, (legend_x, y, 20, 20))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (legend_x, y, 20, 20), 1)\n",
    "            text = self.font_small.render(label, True, self.BLACK)\n",
    "            self.screen.blit(text, (legend_x + 30, y + 2))\n",
    "    \n",
    "    def draw_progress_bar(self):\n",
    "        \"\"\"Dessiner une barre de progression pour l'épisode\"\"\"\n",
    "        if not hasattr(self, 'episode_length'):\n",
    "            self.episode_length = 100  # Longueur maximale estimée\n",
    "            \n",
    "        bar_x = self.world_x\n",
    "        bar_y = self.world_y + self.cell_height + 120\n",
    "        bar_width = self.world.size * self.cell_width\n",
    "        bar_height = 20\n",
    "        \n",
    "        # Fond de la barre\n",
    "        pygame.draw.rect(self.screen, self.GRAY, (bar_x, bar_y, bar_width, bar_height))\n",
    "        \n",
    "        # Progression\n",
    "        if hasattr(self, 'current_step'):\n",
    "            progress = min(1.0, self.current_step / self.episode_length)\n",
    "            pygame.draw.rect(self.screen, self.GREEN, (bar_x, bar_y, bar_width * progress, bar_height))\n",
    "        \n",
    "        # Bordure\n",
    "        pygame.draw.rect(self.screen, self.BLACK, (bar_x, bar_y, bar_width, bar_height), 2)\n",
    "        \n",
    "        # Texte\n",
    "        text = self.font_small.render(\"Progression de l'épisode\", True, self.BLACK)\n",
    "        text_rect = text.get_rect(center=(bar_x + bar_width // 2, bar_y - 15))\n",
    "        self.screen.blit(text, text_rect)\n",
    "    \n",
    "    def update_display(self):\n",
    "        \"\"\"Mettre à jour l'affichage complet\"\"\"\n",
    "        self.screen.fill(self.WHITE)\n",
    "        \n",
    "        self.draw_world()\n",
    "        self.draw_q_values()\n",
    "        self.draw_policy()\n",
    "        self.draw_statistics()\n",
    "        self.draw_legend()\n",
    "        self.draw_model_info()\n",
    "        self.draw_progress_bar()\n",
    "        \n",
    "        # Incrémenter l'animation\n",
    "        self.animation_step += 1\n",
    "        \n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def run_episode(self, max_steps=100, delay=0.1):\n",
    "        \"\"\"Exécuter un épisode avec visualisation\"\"\"\n",
    "        state = self.world.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        self.current_step = 0\n",
    "        self.episode_length = max_steps\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            self.current_step = step\n",
    "            \n",
    "            # Gérer les événements Pygame\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    return False, total_reward, steps\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        # Pause - attendre une autre pression sur espace\n",
    "                        paused = True\n",
    "                        while paused:\n",
    "                            for pause_event in pygame.event.get():\n",
    "                                if pause_event.type == pygame.QUIT:\n",
    "                                    return False, total_reward, steps\n",
    "                                elif pause_event.type == pygame.KEYDOWN:\n",
    "                                    if pause_event.key == pygame.K_SPACE:\n",
    "                                        paused = False\n",
    "                            time.sleep(0.1)\n",
    "            \n",
    "            # Vérifier si l'état est terminal\n",
    "            if self.world.is_terminal(state):\n",
    "                break\n",
    "            \n",
    "            # Sélectionner et exécuter une action\n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = self.agent.select_action(state, valid_actions)\n",
    "            next_state, reward, done = self.world.step(action)\n",
    "            \n",
    "            # Apprentissage Dyna-Q\n",
    "            valid_next_actions = self.world.get_valid_actions(next_state)\n",
    "            self.agent.learn_step(state, action, reward, next_state, \n",
    "                                valid_next_actions, self.world.get_valid_actions)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            self.total_steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # Mettre à jour l'affichage\n",
    "            self.update_display()\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return True, total_reward, steps\n",
    "    \n",
    "    def run_training(self, n_episodes=200, delay=0.1):\n",
    "        \"\"\"Exécuter l'entraînement complet\"\"\"\n",
    "        print(\"=== Démarrage de l'entraînement Dyna-Q ===\")\n",
    "        print(\"Contrôles:\")\n",
    "        print(\"- ESC: Arrêter l'entraînement\")\n",
    "        print(\"- SPACE: Pause/Reprendre\")\n",
    "        print(\"- Fermer la fenêtre: Quitter\")\n",
    "        print()\n",
    "        \n",
    "        running = True\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            if not running:\n",
    "                break\n",
    "                \n",
    "            self.current_episode = episode + 1\n",
    "            \n",
    "            # Exécuter un épisode\n",
    "            continue_training, reward, steps = self.run_episode(delay=delay)\n",
    "            if not continue_training:\n",
    "                break\n",
    "            \n",
    "            # Enregistrer les statistiques\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_steps.append(steps)\n",
    "            \n",
    "            # Réduire progressivement l'exploration\n",
    "            if episode % 20 == 0 and episode > 0:\n",
    "                self.agent.epsilon = max(0.01, self.agent.epsilon * 0.95)\n",
    "            \n",
    "            # Afficher les progrès dans la console\n",
    "            if episode % 50 == 0 or episode < 10:\n",
    "                recent_rewards = self.episode_rewards[-20:] if len(self.episode_rewards) >= 20 else self.episode_rewards\n",
    "                avg_reward = sum(recent_rewards) / len(recent_rewards) if recent_rewards else 0\n",
    "                print(f\"Épisode {episode}: Récompense moyenne = {avg_reward:.3f}, \"\n",
    "                      f\"Epsilon = {self.agent.epsilon:.3f}, Steps = {steps}\")\n",
    "        \n",
    "        print(\"\\n=== Entraînement terminé ===\")\n",
    "        print(f\"Episodes complétés: {len(self.episode_rewards)}\")\n",
    "        if self.episode_rewards:\n",
    "            final_avg = sum(self.episode_rewards[-50:]) / min(50, len(self.episode_rewards))\n",
    "            print(f\"Performance finale (50 derniers épisodes): {final_avg:.3f}\")\n",
    "        \n",
    "        # Afficher un résumé final\n",
    "        input(\"Appuyez sur Entrée pour fermer...\")\n",
    "        pygame.quit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"=== Configuration Dyna-Q LineWorld ===\")\n",
    "    \n",
    "    # Créer l'environnement LineWorld\n",
    "    world_size = 7\n",
    "    start_pos = world_size // 2  # Position centrale\n",
    "    world = LineWorld(size=world_size, start_state=start_pos)\n",
    "    \n",
    "    print(f\"Monde linéaire de taille {world_size}\")\n",
    "    print(f\"Position de départ: S{start_pos}\")\n",
    "    print(f\"États terminaux: S0 (récompense -1) et S{world_size-1} (récompense +1)\")\n",
    "    \n",
    "    # Créer l'agent Dyna-Q\n",
    "    agent = DynaQAgent(\n",
    "        n_actions=2,           # Gauche, Droite\n",
    "        alpha=0.1,             # Taux d'apprentissage\n",
    "        gamma=0.95,            # Facteur de discount\n",
    "        epsilon=0.3,           # Exploration initiale\n",
    "        n_planning=10          # Étapes de planification\n",
    "    )\n",
    "    \n",
    "    print(f\"Agent Dyna-Q configuré:\")\n",
    "    print(f\"- Taux d'apprentissage (α): {agent.alpha}\")\n",
    "    print(f\"- Facteur de discount (γ): {agent.gamma}\")\n",
    "    print(f\"- Exploration initiale (ε): {agent.epsilon}\")\n",
    "    print(f\"- Étapes de planification: {agent.n_planning}\")\n",
    "    \n",
    "    # Créer la visualisation\n",
    "    viz = DynaQLineWorldVisualization(world, agent)\n",
    "    \n",
    "    # Lancer l'entraînement\n",
    "    print(\"\\nLancement de la visualisation...\")\n",
    "    viz.run_training(n_episodes=300, delay=0.08)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234aa9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DynaQAgent:\n",
    "    \"\"\"Algorithme Dyna-Q selon Sutton & Barto\"\"\"\n",
    "    \n",
    "    def __init__(self, n_actions, alpha=0.1, gamma=0.95, epsilon=0.1, n_planning=50):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha  # Taux d'apprentissage\n",
    "        self.gamma = gamma  # Facteur de discount\n",
    "        self.epsilon = epsilon  # Exploration ε-greedy\n",
    "        self.n_planning = n_planning  # Étapes de planification\n",
    "        \n",
    "        # Table Q : Q[état][action] = valeur\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        # Modèle : Model[état][action] = (récompense, nouvel_état)\n",
    "        self.Model = defaultdict(lambda: defaultdict(lambda: None))\n",
    "        \n",
    "        # Paires (état, action) visitées pour planification\n",
    "        self.visited_state_actions = set()\n",
    "    \n",
    "    def select_action(self, state, valid_actions=None):\n",
    "        \"\"\"Politique ε-greedy\"\"\"\n",
    "        if valid_actions is None:\n",
    "            valid_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = [self.Q[state][action] for action in valid_actions]\n",
    "            if not q_values:\n",
    "                return random.choice(valid_actions)\n",
    "            max_q = max(q_values)\n",
    "            best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "    \n",
    "    def update_q(self, state, action, reward, next_state, valid_next_actions=None):\n",
    "        \"\"\"Mise à jour Q-learning\"\"\"\n",
    "        if valid_next_actions is None:\n",
    "            valid_next_actions = list(range(self.n_actions))\n",
    "        \n",
    "        if valid_next_actions:\n",
    "            max_next_q = max([self.Q[next_state][a] for a in valid_next_actions])\n",
    "        else:\n",
    "            max_next_q = 0.0\n",
    "        \n",
    "        # Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]\n",
    "        current_q = self.Q[state][action]\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "        self.Q[state][action] = current_q + self.alpha * (td_target - current_q)\n",
    "    \n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"Mise à jour du modèle\"\"\"\n",
    "        self.Model[state][action] = (reward, next_state)\n",
    "        self.visited_state_actions.add((state, action))\n",
    "    \n",
    "    def planning_step(self, get_valid_actions_fn=None):\n",
    "        \"\"\"Une étape de planification\"\"\"\n",
    "        if not self.visited_state_actions:\n",
    "            return\n",
    "        \n",
    "        # Échantillonner (état, action) aléatoirement\n",
    "        state, action = random.choice(list(self.visited_state_actions))\n",
    "        \n",
    "        if self.Model[state][action] is not None:\n",
    "            reward, next_state = self.Model[state][action]\n",
    "            \n",
    "            # Actions valides dans next_state\n",
    "            if get_valid_actions_fn:\n",
    "                valid_next_actions = get_valid_actions_fn(next_state)\n",
    "            else:\n",
    "                valid_next_actions = list(range(self.n_actions))\n",
    "            \n",
    "            # Mise à jour Q avec expérience simulée\n",
    "            self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "    \n",
    "    def learn_step(self, state, action, reward, next_state, valid_next_actions=None, get_valid_actions_fn=None):\n",
    "        \"\"\"Étape complète Dyna-Q\"\"\"\n",
    "        \n",
    "        # 1. APPRENTISSAGE DIRECT\n",
    "        self.update_q(state, action, reward, next_state, valid_next_actions)\n",
    "        \n",
    "        # 2. MISE À JOUR MODÈLE\n",
    "        self.update_model(state, action, reward, next_state)\n",
    "        \n",
    "        # 3. PLANIFICATION (n étapes)\n",
    "        for _ in range(self.n_planning):\n",
    "            self.planning_step(get_valid_actions_fn)\n",
    "\n",
    "\n",
    "class LineWorld:\n",
    "    def __init__(self, size=5, start_state=2):\n",
    "        \"\"\"\n",
    "        Initialize the LineWorld environment.\n",
    "\n",
    "        Args:\n",
    "            size (int): Number of states in the environment (default 5).\n",
    "            start_state (int): The agent's initial position (default 2).\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.start_state = start_state\n",
    "        self.state = start_state\n",
    "        self.terminal_states = [0, size - 1]\n",
    "        self.action_space = [0, 1]  # 0: left, 1: right\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"For compatibility with RL algos. State is already int, so just return.\"\"\"\n",
    "        return state\n",
    "\n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"For compatibility with RL algos. State is already int, so just return.\"\"\"\n",
    "        return index\n",
    "\n",
    "    @property\n",
    "    def n_states(self):\n",
    "        \"\"\"Total number of states (for tabular RL).\"\"\"\n",
    "        return self.size\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "        Returns:\n",
    "            int: The initial state.\n",
    "        \"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"\n",
    "        Check if a given state is terminal.\n",
    "        Args:\n",
    "            state (int): State to check.\n",
    "        Returns:\n",
    "            bool: True if the state is terminal, False otherwise.\n",
    "        \"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_reward(self, next_state):\n",
    "        \"\"\"\n",
    "        Get the reward for transitioning into the next state.\n",
    "        Args:\n",
    "            next_state (int): The state resulting from the agent's action.\n",
    "        Returns:\n",
    "            float: The reward received for the transition.\n",
    "        \"\"\"\n",
    "        if next_state == 0:\n",
    "            return -1.0\n",
    "        elif next_state == self.size - 1:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def get_valid_actions(self, state):\n",
    "        \"\"\"Get valid actions for a given state\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.action_space.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        Args:\n",
    "            action (int): Action to take (0 = left, 1 = right).\n",
    "        Returns:\n",
    "            tuple: (next_state (int), reward (float), done (bool))\n",
    "        \"\"\"\n",
    "        if self.is_terminal(self.state):\n",
    "            return self.state, 0.0, True\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = max(self.state - 1, 0)\n",
    "        elif action == 1:\n",
    "            next_state = min(self.state + 1, self.size - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action (0=left, 1=right)\")\n",
    "\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_terminal(next_state)\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def simulate_step(self, state, action):\n",
    "        \"\"\"\n",
    "        Simulate a step from a given state and action without modifying self.state.\n",
    "        Returns:\n",
    "            next_state, reward, done\n",
    "        \"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return state, 0.0, True\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = max(state - 1, 0)\n",
    "        elif action == 1:\n",
    "            next_state = min(state + 1, self.size - 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action (0=left, 1=right)\")\n",
    "\n",
    "        reward = self.get_reward(next_state)\n",
    "        done = self.is_terminal(next_state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Print a visual representation of the environment.\n",
    "        'A': Agent position\n",
    "        'T': Terminal state(s)\n",
    "        '_': Normal state\n",
    "        \"\"\"\n",
    "        line = []\n",
    "        for i in range(self.size):\n",
    "            if i == self.state:\n",
    "                line.append('A')\n",
    "            elif i in self.terminal_states:\n",
    "                line.append('T')\n",
    "            else:\n",
    "                line.append('_')\n",
    "        print(' '.join(line))\n",
    "\n",
    "\n",
    "class DynaQLineWorldVisualization:\n",
    "    \"\"\"Visualisation Pygame pour Dyna-Q dans LineWorld\"\"\"\n",
    "    \n",
    "    def __init__(self, world, agent, width=1200, height=700):\n",
    "        pygame.init()\n",
    "        self.world = world\n",
    "        self.agent = agent\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((width, height))\n",
    "        pygame.display.set_caption(\"Dyna-Q LineWorld - Apprentissage par Renforcement\")\n",
    "        \n",
    "        # Couleurs\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.BLUE = (0, 100, 255)\n",
    "        self.RED = (255, 50, 50)\n",
    "        self.GREEN = (50, 255, 50)\n",
    "        self.GRAY = (180, 180, 180)\n",
    "        self.YELLOW = (255, 255, 0)\n",
    "        self.PURPLE = (150, 50, 200)\n",
    "        self.DARK_GREEN = (0, 150, 0)\n",
    "        self.DARK_RED = (150, 0, 0)\n",
    "        self.LIGHT_BLUE = (173, 216, 230)\n",
    "        \n",
    "        # Paramètres de visualisation\n",
    "        self.cell_width = 80\n",
    "        self.cell_height = 80\n",
    "        self.world_y = height // 2 - self.cell_height // 2\n",
    "        self.world_x = (width - world.size * self.cell_width) // 2\n",
    "        \n",
    "        # Polices\n",
    "        self.font_large = pygame.font.Font(None, 28)\n",
    "        self.font_medium = pygame.font.Font(None, 24)\n",
    "        self.font_small = pygame.font.Font(None, 18)\n",
    "        self.font_tiny = pygame.font.Font(None, 14)\n",
    "        \n",
    "        # Statistiques\n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.current_episode = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        # Animation\n",
    "        self.animation_step = 0\n",
    "        \n",
    "    def draw_world(self):\n",
    "        \"\"\"Dessiner le monde linéaire\"\"\"\n",
    "        for i in range(self.world.size):\n",
    "            x = self.world_x + i * self.cell_width\n",
    "            y = self.world_y\n",
    "            \n",
    "            # Couleur de base selon le type de cellule\n",
    "            if i in self.world.terminal_states:\n",
    "                if i == 0:  # Terminal négatif\n",
    "                    base_color = self.DARK_RED\n",
    "                    label_color = self.WHITE\n",
    "                    reward_text = \"-1\"\n",
    "                else:  # Terminal positif\n",
    "                    base_color = self.DARK_GREEN\n",
    "                    label_color = self.WHITE\n",
    "                    reward_text = \"+1\"\n",
    "            else:\n",
    "                base_color = self.LIGHT_BLUE\n",
    "                label_color = self.BLACK\n",
    "                reward_text = \"0\"\n",
    "            \n",
    "            # Effet de surbrillance pour la position actuelle\n",
    "            if i == self.world.state:\n",
    "                # Animation de pulsation\n",
    "                pulse = abs(np.sin(self.animation_step * 0.1)) * 30\n",
    "                base_color = tuple(min(255, c + pulse) for c in base_color)\n",
    "            \n",
    "            # Dessiner la cellule\n",
    "            pygame.draw.rect(self.screen, base_color, (x, y, self.cell_width, self.cell_height))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (x, y, self.cell_width, self.cell_height), 3)\n",
    "            \n",
    "            # Numéro de l'état\n",
    "            state_text = self.font_medium.render(f\"S{i}\", True, label_color)\n",
    "            state_rect = state_text.get_rect(center=(x + self.cell_width // 2, y + 15))\n",
    "            self.screen.blit(state_text, state_rect)\n",
    "            \n",
    "            # Récompense\n",
    "            reward_surface = self.font_small.render(f\"R: {reward_text}\", True, label_color)\n",
    "            reward_rect = reward_surface.get_rect(center=(x + self.cell_width // 2, y + 35))\n",
    "            self.screen.blit(reward_surface, reward_rect)\n",
    "            \n",
    "            # Agent (marqueur spécial)\n",
    "            if i == self.world.state:\n",
    "                agent_radius = 15\n",
    "                pygame.draw.circle(self.screen, self.YELLOW, \n",
    "                                 (x + self.cell_width // 2, y + self.cell_height - 25), \n",
    "                                 agent_radius)\n",
    "                pygame.draw.circle(self.screen, self.BLACK, \n",
    "                                 (x + self.cell_width // 2, y + self.cell_height - 25), \n",
    "                                 agent_radius, 2)\n",
    "                agent_text = self.font_small.render(\"A\", True, self.BLACK)\n",
    "                agent_rect = agent_text.get_rect(center=(x + self.cell_width // 2, y + self.cell_height - 25))\n",
    "                self.screen.blit(agent_text, agent_rect)\n",
    "    \n",
    "    def draw_q_values(self):\n",
    "        \"\"\"Dessiner les valeurs Q sous chaque état\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if self.world.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            x = self.world_x + state * self.cell_width\n",
    "            y = self.world_y + self.cell_height + 10\n",
    "            \n",
    "            # Valeurs Q pour cet état\n",
    "            q_left = self.agent.Q[state][0]\n",
    "            q_right = self.agent.Q[state][1]\n",
    "            \n",
    "            # Couleurs basées sur les valeurs\n",
    "            left_color = self.GREEN if q_left > q_right else self.GRAY\n",
    "            right_color = self.GREEN if q_right > q_left else self.GRAY\n",
    "            \n",
    "            # Afficher Q-values avec couleurs\n",
    "            left_text = f\"←{q_left:.2f}\"\n",
    "            right_text = f\"→{q_right:.2f}\"\n",
    "            \n",
    "            left_surface = self.font_tiny.render(left_text, True, left_color)\n",
    "            right_surface = self.font_tiny.render(right_text, True, right_color)\n",
    "            \n",
    "            # Positionner les textes\n",
    "            left_rect = left_surface.get_rect(center=(x + self.cell_width // 4, y))\n",
    "            right_rect = right_surface.get_rect(center=(x + 3 * self.cell_width // 4, y))\n",
    "            \n",
    "            self.screen.blit(left_surface, left_rect)\n",
    "            self.screen.blit(right_surface, right_rect)\n",
    "    \n",
    "    def draw_policy(self):\n",
    "        \"\"\"Dessiner la politique (flèches pour la meilleure action)\"\"\"\n",
    "        for state in range(self.world.size):\n",
    "            if self.world.is_terminal(state):\n",
    "                continue\n",
    "                \n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                continue\n",
    "            \n",
    "            # Trouver la meilleure action\n",
    "            q_values = [self.agent.Q[state][action] for action in valid_actions]\n",
    "            if max(q_values) == min(q_values):\n",
    "                continue  # Pas de préférence claire\n",
    "                \n",
    "            best_action = valid_actions[np.argmax(q_values)]\n",
    "            \n",
    "            # Dessiner la flèche\n",
    "            x = self.world_x + state * self.cell_width + self.cell_width // 2\n",
    "            y = self.world_y - 30\n",
    "            \n",
    "            arrow_size = 12\n",
    "            if best_action == 0:  # Gauche\n",
    "                points = [(x-arrow_size, y), (x+arrow_size//2, y-arrow_size//2), (x+arrow_size//2, y+arrow_size//2)]\n",
    "                pygame.draw.polygon(self.screen, self.PURPLE, points)\n",
    "            else:  # Droite\n",
    "                points = [(x+arrow_size, y), (x-arrow_size//2, y-arrow_size//2), (x-arrow_size//2, y+arrow_size//2)]\n",
    "                pygame.draw.polygon(self.screen, self.PURPLE, points)\n",
    "    \n",
    "    def draw_model_info(self):\n",
    "        \"\"\"Afficher les informations sur le modèle appris\"\"\"\n",
    "        y_start = self.world_y + self.cell_height + 60\n",
    "        \n",
    "        title = self.font_medium.render(\"Modèle Dyna-Q (échantillon):\", True, self.BLACK)\n",
    "        self.screen.blit(title, (20, y_start))\n",
    "        \n",
    "        # Afficher quelques transitions apprises\n",
    "        displayed = 0\n",
    "        max_display = 6\n",
    "        \n",
    "        # Parcourir correctement le modèle à deux niveaux\n",
    "        for state in list(self.agent.Model.keys())[:max_display]:\n",
    "            if displayed >= max_display:\n",
    "                break\n",
    "                \n",
    "            for action in self.agent.Model[state]:\n",
    "                if displayed >= max_display:\n",
    "                    break\n",
    "                    \n",
    "                transition = self.agent.Model[state][action]\n",
    "                if transition is not None:  # Vérifier qu'il y a bien une transition\n",
    "                    reward, next_state = transition\n",
    "                    action_name = \"←\" if action == 0 else \"→\"\n",
    "                    text = f\"S{state} {action_name} → S{next_state} (R={reward})\"\n",
    "                    surface = self.font_small.render(text, True, self.BLACK)\n",
    "                    self.screen.blit(surface, (20, y_start + 25 + displayed * 20))\n",
    "                    displayed += 1\n",
    "    \n",
    "    def draw_statistics(self):\n",
    "        \"\"\"Dessiner les statistiques détaillées\"\"\"\n",
    "        stats_x = 20\n",
    "        stats_y = 20\n",
    "        \n",
    "        # Titre\n",
    "        title = self.font_large.render(\"Statistiques Dyna-Q\", True, self.BLACK)\n",
    "        self.screen.blit(title, (stats_x, stats_y))\n",
    "        \n",
    "        # Informations de l'épisode\n",
    "        info_lines = [\n",
    "            f\"Épisode: {self.current_episode}\",\n",
    "            f\"Position: S{self.world.state}\",\n",
    "            f\"Steps totaux: {self.total_steps}\",\n",
    "            \"\",\n",
    "            f\"Paramètres:\",\n",
    "            f\"  ε (exploration): {self.agent.epsilon:.3f}\",\n",
    "            f\"  α (apprentissage): {self.agent.alpha:.3f}\",\n",
    "            f\"  γ (discount): {self.agent.gamma:.3f}\",\n",
    "            f\"  Planning steps: {self.agent.n_planning}\",\n",
    "        ]\n",
    "        \n",
    "        for i, line in enumerate(info_lines):\n",
    "            if line:  # Ignorer les lignes vides pour le rendu\n",
    "                text = self.font_small.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (stats_x, stats_y + 35 + i * 18))\n",
    "        \n",
    "        # Statistiques de performance\n",
    "        if self.episode_rewards:\n",
    "            perf_y = stats_y + 35 + len(info_lines) * 18 + 20\n",
    "            \n",
    "            recent_rewards = self.episode_rewards[-20:]  # 20 derniers épisodes\n",
    "            avg_reward = sum(recent_rewards) / len(recent_rewards)\n",
    "            \n",
    "            perf_lines = [\n",
    "                \"Performance:\",\n",
    "                f\"  Récompense moyenne (20 derniers): {avg_reward:.3f}\",\n",
    "                f\"  Dernier épisode: {self.episode_rewards[-1]:.3f}\",\n",
    "                f\"  Steps dernier épisode: {self.episode_steps[-1]}\",\n",
    "                f\"  États-actions explorés: {len(self.agent.visited_state_actions)}\",\n",
    "            ]\n",
    "            \n",
    "            for i, line in enumerate(perf_lines):\n",
    "                text = self.font_small.render(line, True, self.BLACK)\n",
    "                self.screen.blit(text, (stats_x, perf_y + i * 18))\n",
    "    \n",
    "    def draw_legend(self):\n",
    "        \"\"\"Dessiner la légende\"\"\"\n",
    "        legend_x = self.width - 250\n",
    "        legend_y = 20\n",
    "        \n",
    "        title = self.font_medium.render(\"Légende:\", True, self.BLACK)\n",
    "        self.screen.blit(title, (legend_x, legend_y))\n",
    "        \n",
    "        legend_items = [\n",
    "            (\"Agent (A)\", self.YELLOW),\n",
    "            (\"État terminal +1\", self.DARK_GREEN),\n",
    "            (\"État terminal -1\", self.DARK_RED),\n",
    "            (\"État normal\", self.LIGHT_BLUE),\n",
    "            (\"Meilleure action\", self.PURPLE),\n",
    "            (\"Q-value élevée\", self.GREEN),\n",
    "        ]\n",
    "        \n",
    "        for i, (label, color) in enumerate(legend_items):\n",
    "            y = legend_y + 30 + i * 25\n",
    "            pygame.draw.rect(self.screen, color, (legend_x, y, 20, 20))\n",
    "            pygame.draw.rect(self.screen, self.BLACK, (legend_x, y, 20, 20), 1)\n",
    "            text = self.font_small.render(label, True, self.BLACK)\n",
    "            self.screen.blit(text, (legend_x + 30, y + 2))\n",
    "    \n",
    "    def draw_progress_bar(self):\n",
    "        \"\"\"Dessiner une barre de progression pour l'épisode\"\"\"\n",
    "        if not hasattr(self, 'episode_length'):\n",
    "            self.episode_length = 100  # Longueur maximale estimée\n",
    "            \n",
    "        bar_x = self.world_x\n",
    "        bar_y = self.world_y + self.cell_height + 120\n",
    "        bar_width = self.world.size * self.cell_width\n",
    "        bar_height = 20\n",
    "        \n",
    "        # Fond de la barre\n",
    "        pygame.draw.rect(self.screen, self.GRAY, (bar_x, bar_y, bar_width, bar_height))\n",
    "        \n",
    "        # Progression\n",
    "        if hasattr(self, 'current_step'):\n",
    "            progress = min(1.0, self.current_step / self.episode_length)\n",
    "            pygame.draw.rect(self.screen, self.GREEN, (bar_x, bar_y, bar_width * progress, bar_height))\n",
    "        \n",
    "        # Bordure\n",
    "        pygame.draw.rect(self.screen, self.BLACK, (bar_x, bar_y, bar_width, bar_height), 2)\n",
    "        \n",
    "        # Texte\n",
    "        text = self.font_small.render(\"Progression de l'épisode\", True, self.BLACK)\n",
    "        text_rect = text.get_rect(center=(bar_x + bar_width // 2, bar_y - 15))\n",
    "        self.screen.blit(text, text_rect)\n",
    "    \n",
    "    def update_display(self):\n",
    "        \"\"\"Mettre à jour l'affichage complet\"\"\"\n",
    "        self.screen.fill(self.WHITE)\n",
    "        \n",
    "        self.draw_world()\n",
    "        self.draw_q_values()\n",
    "        self.draw_policy()\n",
    "        self.draw_statistics()\n",
    "        self.draw_legend()\n",
    "        self.draw_model_info()\n",
    "        self.draw_progress_bar()\n",
    "        \n",
    "        # Incrémenter l'animation\n",
    "        self.animation_step += 1\n",
    "        \n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def run_episode(self, max_steps=100, delay=0.1):\n",
    "        \"\"\"Exécuter un épisode avec visualisation\"\"\"\n",
    "        state = self.world.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        self.current_step = 0\n",
    "        self.episode_length = max_steps\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            self.current_step = step\n",
    "            \n",
    "            # Gérer les événements Pygame\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    return False, total_reward, steps\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_SPACE:\n",
    "                        # Pause - attendre une autre pression sur espace\n",
    "                        paused = True\n",
    "                        while paused:\n",
    "                            for pause_event in pygame.event.get():\n",
    "                                if pause_event.type == pygame.QUIT:\n",
    "                                    return False, total_reward, steps\n",
    "                                elif pause_event.type == pygame.KEYDOWN:\n",
    "                                    if pause_event.key == pygame.K_SPACE:\n",
    "                                        paused = False\n",
    "                            time.sleep(0.1)\n",
    "            \n",
    "            # Vérifier si l'état est terminal\n",
    "            if self.world.is_terminal(state):\n",
    "                break\n",
    "            \n",
    "            # Sélectionner et exécuter une action\n",
    "            valid_actions = self.world.get_valid_actions(state)\n",
    "            if not valid_actions:\n",
    "                break\n",
    "                \n",
    "            action = self.agent.select_action(state, valid_actions)\n",
    "            next_state, reward, done = self.world.step(action)\n",
    "            \n",
    "            # Apprentissage Dyna-Q\n",
    "            valid_next_actions = self.world.get_valid_actions(next_state)\n",
    "            self.agent.learn_step(state, action, reward, next_state, \n",
    "                                valid_next_actions, self.world.get_valid_actions)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            self.total_steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # Mettre à jour l'affichage\n",
    "            self.update_display()\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return True, total_reward, steps\n",
    "    \n",
    "    def run_training(self, n_episodes=200, delay=0.1):\n",
    "        \"\"\"Exécuter l'entraînement complet\"\"\"\n",
    "        print(\"=== Démarrage de l'entraînement Dyna-Q ===\")\n",
    "        print(\"Contrôles:\")\n",
    "        print(\"- ESC: Arrêter l'entraînement\")\n",
    "        print(\"- SPACE: Pause/Reprendre\")\n",
    "        print(\"- Fermer la fenêtre: Quitter\")\n",
    "        print()\n",
    "        \n",
    "        running = True\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            if not running:\n",
    "                break\n",
    "                \n",
    "            self.current_episode = episode + 1\n",
    "            \n",
    "            # Exécuter un épisode\n",
    "            continue_training, reward, steps = self.run_episode(delay=delay)\n",
    "            if not continue_training:\n",
    "                break\n",
    "            \n",
    "            # Enregistrer les statistiques\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_steps.append(steps)\n",
    "            \n",
    "            # Réduire progressivement l'exploration\n",
    "            if episode % 20 == 0 and episode > 0:\n",
    "                self.agent.epsilon = max(0.01, self.agent.epsilon * 0.95)\n",
    "            \n",
    "            # Afficher les progrès dans la console\n",
    "            if episode % 50 == 0 or episode < 10:\n",
    "                recent_rewards = self.episode_rewards[-20:] if len(self.episode_rewards) >= 20 else self.episode_rewards\n",
    "                avg_reward = sum(recent_rewards) / len(recent_rewards) if recent_rewards else 0\n",
    "                print(f\"Épisode {episode}: Récompense moyenne = {avg_reward:.3f}, \"\n",
    "                      f\"Epsilon = {self.agent.epsilon:.3f}, Steps = {steps}\")\n",
    "        \n",
    "        print(\"\\n=== Entraînement terminé ===\")\n",
    "        print(f\"Episodes complétés: {len(self.episode_rewards)}\")\n",
    "        if self.episode_rewards:\n",
    "            final_avg = sum(self.episode_rewards[-50:]) / min(50, len(self.episode_rewards))\n",
    "            print(f\"Performance finale (50 derniers épisodes): {final_avg:.3f}\")\n",
    "        \n",
    "        # Afficher un résumé final\n",
    "        input(\"Appuyez sur Entrée pour fermer...\")\n",
    "        pygame.quit()\n",
    "\n",
    "    def run_manual_test(self):\n",
    "        \"\"\"Mode manuel : l'utilisateur contrôle l'agent avec le clavier\"\"\"\n",
    "        state = self.world.reset()\n",
    "        self.current_episode = \"MANUEL\"\n",
    "        self.current_step = 0\n",
    "        self.episode_length = 100\n",
    "\n",
    "        running = True\n",
    "        clock = pygame.time.Clock()\n",
    "\n",
    "        print(\"=== MODE TEST MANUEL ===\")\n",
    "        print(\"Touches :\")\n",
    "        print(\"← (flèche gauche) = aller à gauche\")\n",
    "        print(\"→ (flèche droite) = aller à droite\")\n",
    "        print(\"R = Réinitialiser à la position de départ\")\n",
    "        print(\"ESC = Quitter le test\\n\")\n",
    "\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "                    break\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        running = False\n",
    "                        break\n",
    "                    elif event.key == pygame.K_LEFT:\n",
    "                        action = 0\n",
    "                    elif event.key == pygame.K_RIGHT:\n",
    "                        action = 1\n",
    "                    elif event.key == pygame.K_r:\n",
    "                        state = self.world.reset()\n",
    "                        continue\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    if action is not None and not self.world.is_terminal(state):\n",
    "                        next_state, reward, done = self.world.step(action)\n",
    "                        print(f\"Action {'←' if action == 0 else '→'} | S{state} → S{next_state} | R = {reward}\")\n",
    "                        state = next_state\n",
    "\n",
    "                        if done:\n",
    "                            print(f\"🎯 État terminal atteint: S{state} | Récompense: {reward}\")\n",
    "                            print(\"Appuie sur R pour recommencer ou ESC pour quitter.\")\n",
    "\n",
    "            self.update_display()\n",
    "            clock.tick(10)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"=== Configuration Dyna-Q LineWorld ===\")\n",
    "    \n",
    "    # Créer l'environnement LineWorld\n",
    "    world_size = 7\n",
    "    start_pos = world_size // 2\n",
    "    world = LineWorld(size=world_size, start_state=start_pos)\n",
    "    \n",
    "    print(f\"Monde linéaire de taille {world_size}\")\n",
    "    print(f\"Position de départ: S{start_pos}\")\n",
    "    print(f\"États terminaux: S0 (récompense -1) et S{world_size-1} (récompense +1)\")\n",
    "    \n",
    "    # Créer l'agent Dyna-Q\n",
    "    agent = DynaQAgent(\n",
    "        n_actions=2,\n",
    "        alpha=0.1,\n",
    "        gamma=0.95,\n",
    "        epsilon=0.3,\n",
    "        n_planning=10\n",
    "    )\n",
    "    \n",
    "    print(f\"Agent Dyna-Q configuré:\")\n",
    "    print(f\"- Taux d'apprentissage (α): {agent.alpha}\")\n",
    "    print(f\"- Facteur de discount (γ): {agent.gamma}\")\n",
    "    print(f\"- Exploration initiale (ε): {agent.epsilon}\")\n",
    "    print(f\"- Étapes de planification: {agent.n_planning}\")\n",
    "    \n",
    "    # Créer la visualisation\n",
    "    viz = DynaQLineWorldVisualization(world, agent)\n",
    "    \n",
    "    # Lancer l'entraînement\n",
    "    print(\"\\nLancement de la visualisation...\")\n",
    "    viz.run_training(n_episodes=300, delay=0.08)\n",
    "\n",
    "    # Lancer le test manuel après l'entraînement\n",
    "    print(\"\\nSouhaitez-vous tester manuellement le comportement de l'agent ? (y/n)\")\n",
    "    choice = input().strip().lower()\n",
    "    if choice == \"y\":\n",
    "        viz.run_manual_test()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
