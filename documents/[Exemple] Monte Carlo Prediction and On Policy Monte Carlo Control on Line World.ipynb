{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "BL5Ob5x-xHvX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LineWorld MDP"
      ],
      "metadata": {
        "id": "g-GRY9PbwYRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RmNhD4hBwQ1F"
      },
      "outputs": [],
      "source": [
        "S = [0, 1, 2, 3, 4] # agent position in line world\n",
        "A = [0, 1] # 0: Left, 1: Right\n",
        "R = [-1.0, 0.0, 1.0]\n",
        "p = np.zeros((len(S), len(A), len(S), len(R))) # state, action, next_state, reward_index\n",
        "T = [0, 4]\n",
        "\n",
        "p[3, 0, 2, 1] = 1.0\n",
        "p[2, 0, 1, 1] = 1.0\n",
        "p[1, 0, 0, 0] = 1.0\n",
        "\n",
        "p[3, 1, 4, 2] = 1.0\n",
        "p[2, 1, 3, 1] = 1.0\n",
        "p[1, 1, 2, 1] = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative Policy Evaluation"
      ],
      "metadata": {
        "id": "kUWn6mpP4wkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List"
      ],
      "metadata": {
        "id": "wU0tM_7G5EET"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_policy_evaluation(\n",
        "    pi: np.ndarray,\n",
        "    S: List[int],\n",
        "    A: List[int],\n",
        "    R: List[float],\n",
        "    T: List[int], # terminal states\n",
        "    p: np.ndarray,\n",
        "    theta: float = 0.00001,\n",
        "    gamma: float = 0.9999999,\n",
        "):\n",
        "  V = np.random.random((len(S),))\n",
        "  V[T] = 0.0\n",
        "\n",
        "  while True:\n",
        "    delta = 0.0\n",
        "\n",
        "    for s in S:\n",
        "      v = V[s]\n",
        "      total = 0.0\n",
        "      for a in A:\n",
        "        sub_total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            r = R[r_index]\n",
        "            sub_total += p[s, a, s_p, r_index] * (r + gamma * V[s_p])\n",
        "        total += pi[s, a] * sub_total\n",
        "      V[s] = total\n",
        "      abs_diff = np.abs(v - V[s])\n",
        "      delta = np.maximum(delta, abs_diff)\n",
        "\n",
        "    if delta < theta:\n",
        "      break\n",
        "  return V\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xmigJVa54wW7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_always_right = np.zeros((len(S), len(A)))\n",
        "pi_always_right[:, 1] = 1.0"
      ],
      "metadata": {
        "id": "tRaa70Ag81pv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_policy_evaluation(pi_always_right, S, A, R, T, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSyf6sT79M7B",
        "outputId": "80a096d8-2f25-4289-a652-bad036686f2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.       , 0.9999998, 0.9999999, 1.       , 0.       ])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_always_left = np.zeros((len(S), len(A)))\n",
        "pi_always_left[:, 0] = 1.0"
      ],
      "metadata": {
        "id": "YJ7JyvSK9X1y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_policy_evaluation(pi_always_left, S, A, R, T, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmPav2o89jrg",
        "outputId": "a13ca45b-7eb2-4452-8efd-15db855d9da5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.       , -1.       , -0.9999999, -0.9999998,  0.       ])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_uniform_random = np.ones((len(S), len(A))) * 0.5"
      ],
      "metadata": {
        "id": "rUemWfon9mBj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_policy_evaluation(pi_uniform_random, S, A, R, T, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrGLX3Cq9wUq",
        "outputId": "ea3be2f3-9752-4902-c52c-ecf0b89fe3f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00000000e+00, -4.99993279e-01,  6.72123684e-06,  5.00003361e-01,\n",
              "        0.00000000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_weird_random = np.zeros((len(S), len(A)))\n",
        "pi_weird_random[:, 1] = 0.7\n",
        "pi_weird_random[:, 0] = 0.3"
      ],
      "metadata": {
        "id": "as6-AdrX9yHr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_policy_evaluation(pi_weird_random, S, A, R, T, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9YfT1g7-F8d",
        "outputId": "c01f2681-f8f9-4035-c150-c96cc899b245"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.18275208, 0.68965118, 0.90689533, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(0.7 * (0 + 0.9999999 * 0.68965317) + 0.3 * (-1 + 0.99999999 * 0.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCMrwLcP-Hlm",
        "outputId": "e222f521-d745-4c42-dc28-6e7f59e56b87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1827571707242781"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Iteration"
      ],
      "metadata": {
        "id": "LQXxXrqoFw6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(\n",
        "    S: List[int],\n",
        "    A: List[int],\n",
        "    R: List[int],\n",
        "    T: List[int],\n",
        "    p: np.ndarray,\n",
        "    theta: float = 0.00001,\n",
        "    gamma: float = 0.999999,\n",
        "):\n",
        "  V = np.random.random((len(S),))\n",
        "  V[T] = 0.0\n",
        "  pi = np.array([np.random.choice(A) for s in S])\n",
        "  pi[T] = 0\n",
        "\n",
        "  while True:\n",
        "\n",
        "    # Policy Evaluation\n",
        "    while True:\n",
        "      delta = 0.0\n",
        "\n",
        "      for s in S:\n",
        "        v = V[s]\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            r = R[r_index]\n",
        "            total += p[s, pi[s], s_p, r_index] * (r + gamma * V[s_p])\n",
        "        V[s] = total\n",
        "        abs_diff = np.abs(v - V[s])\n",
        "        delta = np.maximum(delta, abs_diff)\n",
        "\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy Improvement\n",
        "\n",
        "    policy_stable = True\n",
        "    for s in S:\n",
        "      old_action = pi[s]\n",
        "      best_a = None\n",
        "      best_a_score = -999999999.99999\n",
        "      for a in A:\n",
        "        score = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            r = R[r_index]\n",
        "            score += p[s, a, s_p, r_index] * (r + gamma * V[s_p])\n",
        "        if best_a is None or score > best_a_score:\n",
        "          best_a = a\n",
        "          best_a_score = score\n",
        "      if best_a != old_action:\n",
        "        policy_stable = False\n",
        "      pi[s] = best_a\n",
        "\n",
        "    if policy_stable:\n",
        "      break\n",
        "\n",
        "  return pi, V"
      ],
      "metadata": {
        "id": "Jo6m1kxf_GkK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_iteration(S, A, R, T, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG9l0oTjICUP",
        "outputId": "6cc9c0a1-c6f1-4c90-be36-8034b1c783f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 1, 1, 0]),\n",
              " array([0.      , 0.999998, 0.999999, 1.      , 0.      ]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloEnv:\n",
        "\n",
        "  def num_states(self) -> int:\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def num_actions(self) -> int:\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def step(self, a: int):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def score(self) -> float:\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def is_game_over(self) -> bool:\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def reset(self):\n",
        "    raise NotImplementedError()\n",
        "\n"
      ],
      "metadata": {
        "id": "nKV2yRTM01Bl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Line World for Monte Carlo algorithms"
      ],
      "metadata": {
        "id": "IWeGC-EFyZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LineWorld(MonteCarloEnv):\n",
        "  def __init__(self):\n",
        "    self.s = 2\n",
        "    self.inner_score = 0.0\n",
        "\n",
        "  def num_states(self) -> int:\n",
        "    return 5\n",
        "\n",
        "  def num_actions(self) -> int:\n",
        "    return 2\n",
        "\n",
        "  def state(self) -> int:\n",
        "    return self.s\n",
        "\n",
        "  def step(self, a: int):\n",
        "    assert(a == 1 or a == 0)\n",
        "    if self.is_game_over():\n",
        "      raise Exception(\"Youpi\")\n",
        "\n",
        "    if a == 0:\n",
        "      self.s -= 1\n",
        "    else:\n",
        "      self.s += 1\n",
        "\n",
        "    if self.s == 0:\n",
        "      self.inner_score -= 1.0\n",
        "    if self.s == 4:\n",
        "      self.inner_score += 1.0\n",
        "\n",
        "  def score(self) -> float:\n",
        "    return self.inner_score\n",
        "\n",
        "  def is_game_over(self) -> bool:\n",
        "    return self.s == 0 or self.s == 4\n",
        "\n",
        "  def reset(self):\n",
        "    self.s = 2\n",
        "    self.inner_score = 0.0\n"
      ],
      "metadata": {
        "id": "yzg3pIz-IEWz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vn90zx6j3FW-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_visit_monte_carlo_prediction(\n",
        "    pi: np.ndarray,\n",
        "    env: MonteCarloEnv,\n",
        "    iterations_count: int,\n",
        "    gamma: float = 0.9999999,\n",
        "):\n",
        "  V = np.random.random((env.num_states(),))\n",
        "  Returns = [[] for s in range(env.num_states())]\n",
        "\n",
        "  all_actions = np.arange(env.num_actions())\n",
        "\n",
        "  for it in tqdm(range(iterations_count)):\n",
        "    env.reset()\n",
        "\n",
        "\n",
        "    trajectory_states = []\n",
        "    trajectory_actions = []\n",
        "    trajectory_rewards = []\n",
        "\n",
        "    while not env.is_game_over():\n",
        "      s = env.state()\n",
        "      a = np.random.choice(all_actions, p=pi[s])\n",
        "\n",
        "      prev_score = env.score()\n",
        "      env.step(a)\n",
        "      r = env.score() - prev_score\n",
        "\n",
        "      trajectory_states.append(s)\n",
        "      trajectory_actions.append(a)\n",
        "      trajectory_rewards.append(r)\n",
        "\n",
        "    terminal_state = env.state()\n",
        "    V[terminal_state] = 0.0\n",
        "\n",
        "    G = 0\n",
        "\n",
        "    for t in reversed(range(len(trajectory_states))):\n",
        "      s_t = trajectory_states[t]\n",
        "      a_t = trajectory_actions[t]\n",
        "      r_t_plus_1 = trajectory_rewards[t]\n",
        "\n",
        "      G = gamma * G + r_t_plus_1\n",
        "\n",
        "      if s_t not in trajectory_states[0:t]:\n",
        "        Returns[s_t].append(G)\n",
        "        V[s_t] = np.mean(Returns[s_t])\n",
        "\n",
        "  return V"
      ],
      "metadata": {
        "id": "D0fnd9Aq0ooE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def better_first_visit_monte_carlo_prediction(\n",
        "    pi: np.ndarray,\n",
        "    env: MonteCarloEnv,\n",
        "    iterations_count: int,\n",
        "    gamma: float = 0.9999999,\n",
        "):\n",
        "  V = np.random.random((env.num_states(),))\n",
        "  Returns = [0.0 for s in range(env.num_states())]\n",
        "  Returns_counts = [0 for s in range(env.num_states())]\n",
        "\n",
        "  all_actions = np.arange(env.num_actions())\n",
        "\n",
        "  trajectory_states = []\n",
        "  trajectory_actions = []\n",
        "  trajectory_rewards = []\n",
        "\n",
        "  for it in tqdm(range(iterations_count)):\n",
        "    env.reset()\n",
        "\n",
        "    trajectory_states.clear()\n",
        "    trajectory_actions.clear()\n",
        "    trajectory_rewards.clear()\n",
        "\n",
        "    while not env.is_game_over():\n",
        "      s = env.state()\n",
        "      a = np.random.choice(all_actions, p=pi[s])\n",
        "\n",
        "      prev_score = env.score()\n",
        "      env.step(a)\n",
        "      r = env.score() - prev_score\n",
        "\n",
        "      trajectory_states.append(s)\n",
        "      trajectory_actions.append(a)\n",
        "      trajectory_rewards.append(r)\n",
        "\n",
        "    terminal_state = env.state()\n",
        "    V[terminal_state] = 0.0\n",
        "\n",
        "    G = 0\n",
        "\n",
        "    for t in reversed(range(len(trajectory_states))):\n",
        "      s_t = trajectory_states[t]\n",
        "      a_t = trajectory_actions[t]\n",
        "      r_t_plus_1 = trajectory_rewards[t]\n",
        "\n",
        "      G = gamma * G + r_t_plus_1\n",
        "\n",
        "      if s_t not in trajectory_states[0:t]:\n",
        "        Returns[s_t] = (Returns[s_t] * Returns_counts[s_t] + G) / (Returns_counts[s_t] + 1)\n",
        "        Returns_counts[s_t] += 1\n",
        "        V[s_t] = Returns[s_t]\n",
        "\n",
        "  return V"
      ],
      "metadata": {
        "id": "bhwsG8TeAjK2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "better_first_visit_monte_carlo_prediction(\n",
        "    pi_uniform_random,\n",
        "    LineWorld(),\n",
        "    100_000,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt0m7tYH1E88",
        "outputId": "6bdfede5-3be9-453e-a853-ea7b6f319478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 60554/100000 [00:16<00:07, 5412.82it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "better_first_visit_monte_carlo_prediction(\n",
        "    pi_always_right,\n",
        "    LineWorld(),\n",
        "    100_000,\n",
        ")"
      ],
      "metadata": {
        "id": "kcNh1pXr6os1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_do_not_evaluate_me = np.zeros((len(S), len(A)))\n",
        "pi_do_not_evaluate_me[1, 1] = 1.0\n",
        "pi_do_not_evaluate_me[2, 0] = 1.0\n",
        "pi_do_not_evaluate_me[3, 1] = 1.0"
      ],
      "metadata": {
        "id": "IhF3uEhDBvYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_policy_evaluation(\n",
        "    pi_do_not_evaluate_me,\n",
        "    S,\n",
        "    A,\n",
        "    R,\n",
        "    T,\n",
        "    p\n",
        ")"
      ],
      "metadata": {
        "id": "T0044HljCpnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT RUN THIS OR IT WILL LOOP TO INFINITY !!!\n",
        "# better_first_visit_monte_carlo_prediction(\n",
        "#     pi_do_not_evaluate_me,\n",
        "#     LineWorld(),\n",
        "#     100_000,\n",
        "# )"
      ],
      "metadata": {
        "id": "fmT-qF6VC_i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_policy_monte_carlo_control(\n",
        "    env: MonteCarloEnv,\n",
        "    iterations_count: int,\n",
        "    gamma: float = 0.999999,\n",
        "    epsilon: float = 0.1,\n",
        "):\n",
        "  pi = (1.0 / env.num_actions()) * np.ones((env.num_states(), env.num_actions()))\n",
        "  Q = np.random.random((env.num_states(), env.num_actions()))\n",
        "  Returns_counts = np.zeros((env.num_states(), env.num_actions()))\n",
        "\n",
        "  trajectory_states = []\n",
        "  trajectory_actions = []\n",
        "  trajectory_rewards = []\n",
        "\n",
        "  all_actions = np.arange(env.num_actions())\n",
        "\n",
        "  for it in tqdm(range(iterations_count)):\n",
        "    env.reset()\n",
        "\n",
        "    trajectory_states.clear()\n",
        "    trajectory_actions.clear()\n",
        "    trajectory_rewards.clear()\n",
        "\n",
        "    while not env.is_game_over():\n",
        "      s = env.state()\n",
        "      a = np.random.choice(all_actions, p=pi[s])\n",
        "\n",
        "      prev_score = env.score()\n",
        "      env.step(a)\n",
        "      r = env.score() - prev_score\n",
        "\n",
        "      trajectory_states.append(s)\n",
        "      trajectory_actions.append(a)\n",
        "      trajectory_rewards.append(r)\n",
        "\n",
        "    terminal_state = env.state()\n",
        "    Q[terminal_state, :] = 0.0\n",
        "\n",
        "    G = 0\n",
        "\n",
        "    for t in reversed(range(len(trajectory_states))):\n",
        "      s_t = trajectory_states[t]\n",
        "      a_t = trajectory_actions[t]\n",
        "      r_t_plus_1 = trajectory_rewards[t]\n",
        "\n",
        "      G = gamma * G + r_t_plus_1\n",
        "\n",
        "      if (s_t, a_t) not in zip(trajectory_states[0:t], trajectory_actions[0: t]):\n",
        "        Q[s_t, a_t] = (Q[s_t, a_t] * Returns_counts[s_t, a_t] + G) / (Returns_counts[s_t, a_t] + 1)\n",
        "        Returns_counts[s_t] += 1\n",
        "        best_a = np.argmax(Q[s_t])\n",
        "\n",
        "        pi[s_t, :] = epsilon / env.num_actions()\n",
        "        pi[s_t, best_a] = 1.0 - epsilon + epsilon / env.num_actions()\n",
        "  return pi, Q"
      ],
      "metadata": {
        "id": "DDwZYiOeDHid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "on_policy_monte_carlo_control(LineWorld(), 1_000_000)"
      ],
      "metadata": {
        "id": "h_BkpQG1Qwcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBftCeHwQ8Fz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}